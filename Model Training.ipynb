{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f5d47c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415ab001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "import evaluate\n",
    "\n",
    "import wandb\n",
    "# wandb.init(project=\"kg-lm-integration\", entity=\"tanny411\")\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "emb_tsv_file = \"wikidata_translation_v1.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"distilbert-base-uncased\" ##\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a6ac",
   "metadata": {},
   "source": [
    "# Initialize Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8ed2b",
   "metadata": {},
   "source": [
    "Download:\n",
    "- embeds_wktxt.csv\n",
    "- [linked-wikitext-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/) and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41ccbd",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495d87e",
   "metadata": {},
   "source": [
    "- `tokens` are the given list of tokens from wikitext2\n",
    "- `input_ids` are what come from tokenization, they divide certain words into multiple pieces, and each sentence has a CLS and a SEP\n",
    "- `word_tokens` is the length of `tokens`. For each token in `token`, it mentions how many sub-words it was divided into due to word piece tokenization\n",
    "- `cummulative_word_tokens` is a cummulative sum of `word_tokens`, with an extra 0 in the beginning\n",
    "\n",
    "##### Process\n",
    "index of a token in `token` can be found in `input_ids` by `cummulative_word_tokens`. if `ix` is the index of a word in `token`, its beginning index in `input_ids` is `cummulative_word_tokens[ix] + 1`, the +1 is because `input_ids` has a CLS in the beginning. `token[ix]` spans from `input_ids[cummulative_word_tokens[ix] + 1]` to `input_ids[cummulative_word_tokens[ix+1] + 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efb55fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds_wktxt = pd.read_csv(\"embeds_wktxt.csv\")\n",
    "\n",
    "linked_wikitext_2 = \"linked-wikitext-2/\"\n",
    "train = linked_wikitext_2+\"train.jsonl\"\n",
    "valid = linked_wikitext_2+\"valid.jsonl\"\n",
    "test = linked_wikitext_2+\"test.jsonl\"\n",
    "\n",
    "data_files = {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "wikitest2_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "class BertTokenizerModified(DistilBertTokenizer): #BertTokenizer\n",
    "    def __init__(self,vocab_file,**kwargs):\n",
    "        \n",
    "        super().__init__(vocab_file, never_split=[\"@@START@@\", \"@@END@@\"], **kwargs)\n",
    "    \n",
    "        self.tokenized_list = []\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        token_list = text.split()\n",
    "        split_tokens = []\n",
    "        tokenized_list = []\n",
    "        \n",
    "        if self.do_basic_tokenize:\n",
    "            for token in token_list:\n",
    "\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                    tokenized_list.append(1)\n",
    "                else:\n",
    "                    word_tokenized = self.wordpiece_tokenizer.tokenize(token)\n",
    "                    split_tokens += word_tokenized\n",
    "                    tokenized_list.append(len(word_tokenized))\n",
    "\n",
    "        self.tokenized_list.append(tokenized_list)\n",
    "        return split_tokens\n",
    "    \n",
    "def get_cumm(vals):\n",
    "    cumm = 0\n",
    "    res = [0] ## len of res is 1 more than vals, with an initial 0\n",
    "    for val in vals:\n",
    "        cumm += val\n",
    "        res.append(cumm)\n",
    "    return res\n",
    "\n",
    "def my_tokenize_function(data):\n",
    "    \n",
    "    ## tokenize\n",
    "    my_tokenizer.tokenized_list = []\n",
    "    result = my_tokenizer([\" \".join(eg) for eg in data[\"tokens\"]])\n",
    "    if my_tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    ## save word to token mapping\n",
    "    ## 3, 1, 1 means the first word got divided into 3 tokens, the next into 1, and the next into 1 again\n",
    "    result[\"word_tokens\"] = my_tokenizer.tokenized_list\n",
    "    result[\"cummulative_word_tokens\"] = [get_cumm(x) for x in result[\"word_tokens\"]]\n",
    "    return result\n",
    "\n",
    "def get_kg_embedding_batched(data):\n",
    "    \"\"\"\n",
    "    When you specify batched=True the function receives a dictionary with the fields of the dataset, \n",
    "    but each value is now a list of values, and not just a single value. \n",
    "    \"\"\"\n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    annotations_list = data['annotations']\n",
    "    cummulative_word_tokens_list = data[\"cummulative_word_tokens\"]\n",
    "    \n",
    "    batch_size = len(input_ids_list)\n",
    "    embed_list = [] ## len will be batch_size\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        input_ids = input_ids_list[i]\n",
    "        annotations = annotations_list[i]\n",
    "        embeds = np.zeros((len(input_ids), 200)) ## CLS, SEP will have np.zeros, like unknown words\n",
    "        \n",
    "        for annot in annotations:\n",
    "            start_ix, end_ix = annot['span']\n",
    "            start = cummulative_word_tokens_list[i][start_ix] + 1\n",
    "            end = cummulative_word_tokens_list[i][end_ix] + 1\n",
    "            \n",
    "            qid = annot['id']\n",
    "            df = embeds_wktxt[embeds_wktxt['id']==qid]\n",
    "            if len(df)>0:\n",
    "                embeds[start:end] = np.tile(df.iloc[0,1:].values.reshape((1,200)),(end-start, 1))\n",
    "        \n",
    "        embed_list.append(embeds)\n",
    "\n",
    "    return {\"kg_embedding\": embed_list}\n",
    "\n",
    "def filter_text_batched(data):\n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    kg_embeds_list = data['kg_embedding']\n",
    "    \n",
    "    indices_list = [[i for i,input_id in enumerate(input_ids) if input_id!=100]\n",
    "                        for input_ids in input_ids_list]\n",
    "    new_input_ids_list = []\n",
    "    new_kg_embedding_list = []\n",
    "        \n",
    "    for indices, input_ids, kg_embeds in zip(indices_list, input_ids_list, kg_embeds_list):\n",
    "        \n",
    "        new_input_ids = [input_ids[ind] for ind in indices]\n",
    "        new_kg_embeds = [kg_embeds[ind] for ind in indices]\n",
    "        \n",
    "        new_input_ids_list.append(new_input_ids)\n",
    "        new_kg_embedding_list.append(new_kg_embeds)\n",
    "        \n",
    "    return {\"input_ids\": new_input_ids_list, \"kg_embedding\": new_kg_embedding_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4456d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokenized_kg_wikitest2_dataset_file = \"filtered_tokenized_kg_wikitest2_dataset_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to perform tokenization.\n",
    "\n",
    "my_tokenizer = BertTokenizerModified.from_pretrained(bert_model_name)\n",
    "\n",
    "tokenized_kg_wikitest2_dataset = wikitest2_dataset.map(my_tokenize_function, batched=True)\\\n",
    "                                                  .map(get_kg_embedding_batched, \n",
    "                                                         batched=True, \n",
    "                                                         batch_size=100, \n",
    "                                                         keep_in_memory=False)\\\n",
    "                                                  .map(filter_text_batched, \n",
    "                                                         batched=True, \n",
    "                                                         batch_size=100, \n",
    "                                                         keep_in_memory=False)\n",
    "\n",
    "filtered_tokenized_kg_wikitest2_dataset.save_to_disk(filtered_tokenized_kg_wikitest2_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved tokenized dataset\n",
    "dataset = load_from_disk(filtered_tokenized_kg_wikitest2_dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2df1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152f07cc",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d018e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # Create a new labels column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a7e57",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModified(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.config = self.base_model.config\n",
    "        \n",
    "        self.activation = GELUActivation() # for distilbert\n",
    "        self.vocab_transform = nn.Linear(self.config.dim, self.config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(self.config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "\n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ## set to eval\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        ## freeze model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kg_embedding = None,\n",
    "        input_ids = None,\n",
    "        attention_mask = None,\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,):\n",
    "        \n",
    "        base_model_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        ## Get LM embedding\n",
    "        hidden_states = base_model_output[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "        ## TODO: Use hidden_states and kg_embedding and perform INTEGRATION\n",
    "        prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=mlm_loss,\n",
    "            logits=prediction_logits,\n",
    "            hidden_states=base_model_output.hidden_states,\n",
    "            attentions=base_model_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac2e8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BERTModified_model = BERTModified(bert_model_name)\n",
    "data_collator = DataCollatorForWholeWordMask(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403d183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To test model with smaller sample dataset\n",
    "\n",
    "# train_size = 1000\n",
    "# test_size = 100\n",
    "\n",
    "# downsampled_dataset = dataset.train_test_split(train_size=train_size, test_size=test_size, seed=42)\n",
    "# downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c77502",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = evaluate.load(\"perplexity\") # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8d2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset['train']) // batch_size\n",
    "model_name = \"BERTModified\"\n",
    "output_dir = f\"{model_name}-finetuned-wikitext-test\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "#     fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=1,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"loss\",#metric_name,\n",
    "#     greater_is_better = False,\n",
    "    logging_dir='logs',\n",
    "    report_to=\"wandb\",\n",
    "#     no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ed9ae",
   "metadata": {},
   "source": [
    "metric_for_best_model (str, optional) — Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\". Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "\n",
    "If you set this value, greater_is_better will default to True. Don’t forget to set it to False if your metric is better when lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd221993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=BERTModified_model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3305e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cfd7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "# trainer.save_model(\"output/models/BERTModified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ed6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") #21596 for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec0488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Need to login to huggingface to push to hub\n",
    "\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize MLM pipeline\n",
    "mlm = pipeline('fill-mask', model=BERTModified_model, tokenizer=my_tokenizer)\n",
    "\n",
    "# Get mask token\n",
    "mask = mlm.tokenizer.mask_token\n",
    "\n",
    "# Get result for particular masked phrase\n",
    "phrase = f'Wikipedia is a free online {mask}, created and edited by volunteers around the world'\n",
    "\n",
    "result = mlm(phrase)\n",
    "\n",
    "# Print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in result:\n",
    "    print(f\">>> {x['sequence']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
