{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f5d47c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415ab001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers.data.data_collator import _torch_collate_batch\n",
    "import evaluate\n",
    "\n",
    "import wandb\n",
    "# wandb.init(project=\"kg-lm-integration\", entity=\"tanny411\")\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "emb_tsv_file = \"wikidata_translation_v1.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d2eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "# but doing so is dangerous -- the standard limit is a little conservative, but Python stackframes can be quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed5a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a1447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"distilbert-base-uncased\" ##\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a6ac",
   "metadata": {},
   "source": [
    "# Initialize Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8ed2b",
   "metadata": {},
   "source": [
    "Download:\n",
    "- embeds_wktxt.csv\n",
    "- [linked-wikitext-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/) and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41ccbd",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495d87e",
   "metadata": {},
   "source": [
    "- `tokens` are the given list of tokens from wikitext2\n",
    "- `input_ids` are what come from tokenization, they divide certain words into multiple pieces, and each sentence has a CLS and a SEP\n",
    "- `word_tokens` is the length of `tokens`. For each token in `token`, it mentions how many sub-words it was divided into due to word piece tokenization\n",
    "- `cummulative_word_tokens` is a cummulative sum of `word_tokens`, with an extra 0 in the beginning\n",
    "\n",
    "##### Process\n",
    "index of a token in `token` can be found in `input_ids` by `cummulative_word_tokens`. if `ix` is the index of a word in `token`, its beginning index in `input_ids` is `cummulative_word_tokens[ix] + 1`, the +1 is because `input_ids` has a CLS in the beginning. `token[ix]` spans from `input_ids[cummulative_word_tokens[ix] + 1]` to `input_ids[cummulative_word_tokens[ix+1] + 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4efb55fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c5571b3a8bc0c3d4\n",
      "Found cached dataset json (/home/a2khatun/.cache/huggingface/datasets/json/default-c5571b3a8bc0c3d4/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970b0ab9a8d64cefaad8fd1395f486da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b38e982ae37a4744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/a2khatun/.cache/huggingface/datasets/json/default-b38e982ae37a4744/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2375620c824b058faaa3793cc8bce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce595103225647929860076a4b5d6b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/a2khatun/.cache/huggingface/datasets/json/default-b38e982ae37a4744/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d83c5c9297ba4199ac571d5a51ed87af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeds_wktxt = pd.read_csv(\"embeds_wktxt.csv\")\n",
    "\n",
    "linked_wikitext_2 = \"linked-wikitext-2/\"\n",
    "train = linked_wikitext_2+\"train.jsonl\"\n",
    "valid = linked_wikitext_2+\"valid.jsonl\"\n",
    "test = linked_wikitext_2+\"test.jsonl\"\n",
    "synthetic_data = \"generate_test_data/sythetic_dataset.jsonl\"\n",
    "\n",
    "data_files = {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "wikitest2_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "synthetic_dataset = load_dataset(\"json\", data_files={\"synthetic\": synthetic_data})\n",
    "    \n",
    "chunk_size = 128\n",
    "\n",
    "class BertTokenizerModified(DistilBertTokenizer): #BertTokenizer\n",
    "    def __init__(self,vocab_file,**kwargs):\n",
    "        \n",
    "        super().__init__(vocab_file, never_split=[\"@@START@@\", \"@@END@@\", \"@@start@@\", \"@@end@@\"], **kwargs)\n",
    "    \n",
    "        self.tokenized_list = []\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        token_list = text.split()\n",
    "        split_tokens = []\n",
    "        tokenized_list = []\n",
    "        \n",
    "        if self.do_basic_tokenize:\n",
    "            for token in token_list:\n",
    "\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                    tokenized_list.append(1)\n",
    "                else:\n",
    "                    word_tokenized = self.wordpiece_tokenizer.tokenize(token)\n",
    "                    split_tokens += word_tokenized\n",
    "                    tokenized_list.append(len(word_tokenized))\n",
    "\n",
    "        self.tokenized_list.append(tokenized_list)\n",
    "        return split_tokens\n",
    "    \n",
    "def get_cumm(vals):\n",
    "    cumm = 0\n",
    "    res = [0] ## len of res is 1 more than vals, with an initial 0\n",
    "    for val in vals:\n",
    "        cumm += val\n",
    "        res.append(cumm)\n",
    "    return res\n",
    "\n",
    "def my_tokenize_function(data):\n",
    "    \n",
    "    ## tokenize\n",
    "    my_tokenizer.tokenized_list = []\n",
    "    result = my_tokenizer([\" \".join(eg) for eg in data[\"tokens\"]])\n",
    "    if my_tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    ## save word to token mapping\n",
    "    ## 3, 1, 1 means the first word got divided into 3 tokens, the next into 1, and the next into 1 again\n",
    "    result[\"word_tokens\"] = my_tokenizer.tokenized_list\n",
    "    result[\"cummulative_word_tokens\"] = [get_cumm(x) for x in result[\"word_tokens\"]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_kg_embedding_batched(data):\n",
    "    \n",
    "    ## store a masking array that says whether or not an item has kg embedding\n",
    "    \"\"\"\n",
    "    When you specify batched=True the function receives a dictionary with the fields of the dataset, \n",
    "    but each value is now a list of values, and not just a single value. \n",
    "    \"\"\"\n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    annotations_list = data['annotations']\n",
    "    cummulative_word_tokens_list = data[\"cummulative_word_tokens\"]\n",
    "    \n",
    "    batch_size = len(input_ids_list)\n",
    "    embed_list = [] ## len will be batch_size\n",
    "    embed_mask = []\n",
    "    embed_mask_qid = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        input_ids = input_ids_list[i]\n",
    "        annotations = annotations_list[i]\n",
    "        \n",
    "        ## Replace zeros with random numbers if required\n",
    "        embeds = np.zeros((len(input_ids), 200)) ## CLS, SEP will have np.zeros, like unknown words\n",
    "        mask = [0]*len(input_ids)\n",
    "        mask_qid = ['0']*len(input_ids)\n",
    "        \n",
    "        for annot in annotations:\n",
    "            start_ix, end_ix = annot['span']\n",
    "            start = cummulative_word_tokens_list[i][start_ix] + 1\n",
    "            end = cummulative_word_tokens_list[i][end_ix] + 1\n",
    "            \n",
    "            qid = annot['id']\n",
    "            df = embeds_wktxt[embeds_wktxt['id']==qid]\n",
    "            if len(df)>0:\n",
    "                embeds[start:end] = np.tile(df.iloc[0,1:].values.reshape((1,200)),(end-start, 1))\n",
    "                mask[start:end] = [1]*(end-start)\n",
    "                mask_qid[start:end] = [qid]*(end-start)\n",
    "                \n",
    "        embed_mask.append(mask)\n",
    "        embed_mask_qid.append(mask_qid)\n",
    "        embed_list.append(embeds)\n",
    "\n",
    "    return {\n",
    "        \"kg_embedding\": embed_list, \n",
    "        \"kg_embedding_mask\": embed_mask,\n",
    "        \"kg_embedding_mask_qid\": embed_mask_qid\n",
    "    }\n",
    "\n",
    "def filter_text_batched(data):\n",
    "    \n",
    "    new_data = {k:[] for k in data}\n",
    "    \n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    \n",
    "    ## remove [UNK] == 100 \n",
    "    indices_list = [[i for i,input_id in enumerate(input_ids) if input_id!=100]\n",
    "                        for input_ids in input_ids_list]\n",
    "    \n",
    "    for k in data:\n",
    "        for indices, data_list in zip(indices_list, data[k]):\n",
    "            new_data[k].append([data_list[ind] for ind in indices])\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def truncate_data(data):\n",
    "    maxlength = my_tokenizer.max_model_input_sizes[bert_model_name]\n",
    "\n",
    "    ## truncate to maxlength\n",
    "    for k in data:\n",
    "        data[k] = [x[:maxlength] for x in data[k]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Create a new labels column\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ae3812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerModified'.\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = BertTokenizerModified.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b2f2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"concat_dataset_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f538eaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dataset = wikitest2_dataset.map(my_tokenize_function, batched=True)\\\n",
    "                          .map(get_kg_embedding_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .remove_columns(['title', 'tokens', 'annotations', 'word_tokens', 'cummulative_word_tokens'])\\\n",
    "                          .map(filter_text_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .map(group_texts, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "\n",
    "final_dataset.save_to_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fae5e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved tokenized dataset\n",
    "final_dataset = load_from_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b036f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 17528\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 1798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 2069\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "833ed65e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89780c6671f4ac5881bee93834ff083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65df2aa1ff304f278526846642ea341b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_file = \"tokenized_synthetic_dataset\"\n",
    "\n",
    "tokenized_synthetic_dataset = synthetic_dataset.map(my_tokenize_function, batched=True)\\\n",
    "                          .map(get_kg_embedding_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .remove_columns(['title', 'tokens', 'annotations', 'word_tokens', 'cummulative_word_tokens'])\\\n",
    "#                           .map(filter_text_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "#                           .map(group_texts, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "\n",
    "tokenized_synthetic_dataset.save_to_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ce6e817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    synthetic: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_synthetic_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1cf80a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    for key in tokenized_synthetic_dataset[\"synthetic\"].features:\n",
    "        print(tokenized_synthetic_dataset[\"synthetic\"][key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2314fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Data Collator for Masking\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class CustomDataCollator(DataCollatorForWholeWordMask):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "            kg_embedding_mask = [e[\"kg_embedding_mask\"] for e in examples]\n",
    "            kg_embedding = [e[\"kg_embedding\"] for e in examples]\n",
    "            kg_embedding_mask_qid = [e[\"kg_embedding_mask_qid\"] for e in examples]\n",
    "        else:\n",
    "            raise Exception(\"Dataset needs to be in dictionary format\")\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = kg_embedding_mask\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        \n",
    "        return {\n",
    "                \"input_ids\": inputs, \n",
    "                \"labels\": labels, \n",
    "                \"kg_embedding\":kg_embedding, \n",
    "                \"kg_embedding_mask\":kg_embedding_mask,\n",
    "                \"kg_embedding_mask_qid\":kg_embedding_mask_qid,\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a7e57",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class BERTModified(nn.Module): #PreTrainedModel\n",
    "    def __init__(self, bert_model_name, base_model, config):\n",
    "        \n",
    "#         super().__init__(config) #For PreTrainedModel\n",
    "        super().__init__() ## for nn.Module\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.config = config\n",
    "                \n",
    "        self.activation = GELUActivation() # for distilbert\n",
    "        self.vocab_transform = nn.Linear(self.config.dim, self.config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(self.config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "\n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ## set to eval\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        ## freeze model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kg_embedding = None,           ## given\n",
    "        kg_embedding_mask = None,      ## given\n",
    "        kg_embedding_mask_qid = None,      ## given\n",
    "        input_ids = None,              ## given\n",
    "        attention_mask = None,         ## given\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,                 ## given\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,):\n",
    "        \n",
    "        base_model_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        ## Get LM embedding\n",
    "        hidden_states = base_model_output[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "        ## TODO: Use hidden_states and kg_embedding and perform INTEGRATION\n",
    "        prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=mlm_loss,\n",
    "            logits=prediction_logits,\n",
    "            hidden_states=base_model_output.hidden_states,\n",
    "            attentions=base_model_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ac2e8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "BERTModified_model = BERTModified(bert_model_name = bert_model_name,\n",
    "                                  base_model = base_model,\n",
    "                                  config = base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0338cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTModified_model = AutoModel.from_pretrained(\"Aisha/BERTModified-finetuned-wikitext-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2e30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollator(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "# DataCollatorForWholeWordMask(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "403d183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at concat_dataset_v2/train/cache-4211a33408ac86a5.arrow and concat_dataset_v2/train/cache-720f0802bdea057f.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To test model with smaller sample dataset\n",
    "\n",
    "train_size = 100\n",
    "test_size = 10\n",
    "\n",
    "downsampled_dataset = final_dataset[\"train\"].train_test_split(train_size=train_size, test_size=test_size, seed=42)\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25c77502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# metrics = evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds=None, logits=None, labels=None):\n",
    "    \n",
    "    # We should have either `eval_preds` or both `logits` and `labels`\n",
    "    if eval_preds:\n",
    "        logits, labels = eval_preds\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    ## Flatten values\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a8d2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a2khatun/.venv/analysis/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset['train']) // batch_size #len(final_dataset['train']) // batch_size\n",
    "model_name = \"BERTModified\"\n",
    "output_dir = f\"{model_name}-finetuned-wikitext-test\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "#     fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=1,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"loss\",#metric_name,\n",
    "#     greater_is_better = False,\n",
    "    logging_dir='logs',\n",
    "    report_to=\"wandb\",\n",
    "#     no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ed9ae",
   "metadata": {},
   "source": [
    "metric_for_best_model (str, optional) — Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\". Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "\n",
    "If you set this value, greater_is_better will default to True. Don’t forget to set it to False if your metric is better when lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd221993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a2khatun/Downloads/KG/project/BERTModified-finetuned-wikitext-test is already a clone of https://huggingface.co/Aisha/BERTModified-finetuned-wikitext-test. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=BERTModified_model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"], #final_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"], #final_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b027762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [trainer.eval_dataset[i] for i in range(2)]\n",
    "# dc = data_collator(samples)\n",
    "# kg_embedding_mask = [sample['kg_embedding_mask'] for sample in samples]\n",
    "\n",
    "# for id_list, label_list, mask_list in zip(dc[\"input_ids\"], dc[\"labels\"], kg_embedding_mask):\n",
    "#     tokens = my_tokenizer.convert_ids_to_tokens(id_list)\n",
    "#     labels = my_tokenizer.convert_ids_to_tokens(label_list)\n",
    "#     for token, label, mask in zip(tokens, labels, mask_list):\n",
    "#         print(token, '\\t', label, '\\t', mask)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5f89c",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a3305e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtanny411\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/a2khatun/Downloads/KG/project/wandb/run-20221204_012835-2024vnlv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tanny411/huggingface/runs/2024vnlv\" target=\"_blank\">BERTModified-finetuned-wikitext-test</a></strong> to <a href=\"https://wandb.ai/tanny411/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 38112.84\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b7f5033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 10.548306465148926,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 0.9264,\n",
       " 'eval_samples_per_second': 10.794,\n",
       " 'eval_steps_per_second': 3.238}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49714c",
   "metadata": {},
   "source": [
    "# TODO: Trainer.model is not a `PreTrainedModel`, only saving its state dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12cfd7d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a2khatun/.venv/analysis/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 25\n",
      "  Number of trainable parameters = 24063546\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.461300</td>\n",
       "      <td>10.482824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-25\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=10.46131591796875, metrics={'train_runtime': 29.1341, 'train_samples_per_second': 3.432, 'train_steps_per_second': 0.858, 'total_flos': 0.0, 'train_loss': 10.46131591796875, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "# trainer.save_model(\"output/models/BERTModified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b4ed6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 36000.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 10.491297721862793,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 0.9095,\n",
       " 'eval_samples_per_second': 10.995,\n",
       " 'eval_steps_per_second': 3.298,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") #21596 for 1 epoch\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec0488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Need to login to huggingface to push to hub\n",
    "\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd9be2",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ebb2e81f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 128, 30522) (10, 128)\n"
     ]
    }
   ],
   "source": [
    "## errors in GPU (then pc cant find GPU anymore)\n",
    "predictions = trainer.predict(downsampled_dataset[\"test\"]) #final_dataset[\"valid\"] cpu crashes\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf058556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8976, 18199,  8916, ..., 25969, 23858, 18199],\n",
       "       [ 4463, 10208,  4463, ..., 22102,  4463, 17147],\n",
       "       [17912, 13809,  7182, ..., 11629, 27497, 17912],\n",
       "       ...,\n",
       "       [21459,    54, 17240, ...,  5523, 12345, 13532],\n",
       "       [14133,  1076, 19017, ...,  8466, 28247, 19711],\n",
       "       [11487,  6334, 29788, ..., 14683,  6194, 28165]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bfc00b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  8540, 12256, 26775],\n",
       "       ...,\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [15756,  6125,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f585e",
   "metadata": {},
   "source": [
    "[About micro, macro, weighted precision, recall, f1 for multiclass labels](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)\n",
    "\n",
    "The following always holds true for the micro-F1 case:\n",
    "\n",
    "`micro-F1 = micro-precision = micro-recall = accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de842903",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits = predictions.predictions, labels = predictions.label_ids)\n",
    "# metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1a37b",
   "metadata": {},
   "source": [
    "## Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5127cabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 100\n",
    "test_size = 10\n",
    "\n",
    "downsampled_dataset2 = tokenized_synthetic_dataset[\"synthetic\"].train_test_split(train_size=train_size, test_size=test_size, seed=42)\n",
    "downsampled_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b5d998c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 24, 30522) (10, 24)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(downsampled_dataset2[\"test\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05bb3bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits = predictions.predictions, labels = predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f15f3",
   "metadata": {},
   "source": [
    "## Try from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize MLM pipeline\n",
    "mlm = pipeline('fill-mask', model=BERTModified_model, tokenizer=my_tokenizer)\n",
    "\n",
    "# Get mask token\n",
    "mask = mlm.tokenizer.mask_token\n",
    "\n",
    "# Get result for particular masked phrase\n",
    "phrase = f'Wikipedia is a free online {mask}, created and edited by volunteers around the world'\n",
    "\n",
    "result = mlm(phrase)\n",
    "\n",
    "# Print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in result:\n",
    "    print(f\">>> {x['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c32330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6883f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Add a layer for KG embedding, add the losses\n",
    "- new tokenizer for test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
