{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f5d47c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "415ab001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from models.models import *\n",
    "\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers.data.data_collator import _torch_collate_batch\n",
    "import evaluate\n",
    "import wandb\n",
    "from huggingface_hub import notebook_login\n",
    "#from transformers import PreTrainedModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "emb_tsv_file = \"wikidata_translation_v1.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d2eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "# but doing so is dangerous -- the standard limit is a little conservative, but Python stackframes can be quite big.\n",
    "\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a1447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"distilbert-base-uncased\" ##\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e91bdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, DistilBertForMaskedLM\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# model = DistilBertForMaskedLM.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits = model(**inputs).logits\n",
    "\n",
    "# # retrieve index of [MASK]\n",
    "# mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "# predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "\n",
    "# labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# # mask labels of non-[MASK] tokens\n",
    "# labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "# outputs = model(**inputs, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a6ac",
   "metadata": {},
   "source": [
    "# Initialize Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8ed2b",
   "metadata": {},
   "source": [
    "Download:\n",
    "- embeds_wktxt.csv\n",
    "- [linked-wikitext-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/) and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41ccbd",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495d87e",
   "metadata": {},
   "source": [
    "- `tokens` are the given list of tokens from wikitext2\n",
    "- `input_ids` are what come from tokenization, they divide certain words into multiple pieces, and each sentence has a CLS and a SEP\n",
    "- `word_tokens` is the length of `tokens`. For each token in `token`, it mentions how many sub-words it was divided into due to word piece tokenization\n",
    "- `cummulative_word_tokens` is a cummulative sum of `word_tokens`, with an extra 0 in the beginning\n",
    "\n",
    "##### Process\n",
    "index of a token in `token` can be found in `input_ids` by `cummulative_word_tokens`. if `ix` is the index of a word in `token`, its beginning index in `input_ids` is `cummulative_word_tokens[ix] + 1`, the +1 is because `input_ids` has a CLS in the beginning. `token[ix]` spans from `input_ids[cummulative_word_tokens[ix] + 1]` to `input_ids[cummulative_word_tokens[ix+1] + 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4efb55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-07015f1fc3b429ce\n",
      "Found cached dataset json (/home/xiangru/.cache/huggingface/datasets/json/default-07015f1fc3b429ce/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137eed3b92ab461ebcf161b24971386a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeds_wktxt = pd.read_csv(\"embeds_wktxt.csv\")\n",
    "qids_wktxt = pd.read_csv(\"qids_wktxt2.csv\")\n",
    "\n",
    "linked_wikitext_2 = \"linked-wikitext-2/\"\n",
    "train = linked_wikitext_2+\"train.jsonl\"\n",
    "valid = linked_wikitext_2+\"valid.jsonl\"\n",
    "test = linked_wikitext_2+\"test.jsonl\"\n",
    "\n",
    "data_files = {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "wikitest2_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "chunk_size = 128\n",
    "\n",
    "class BertTokenizerModified(DistilBertTokenizer): #BertTokenizer\n",
    "    def __init__(self,vocab_file,**kwargs):\n",
    "        \n",
    "        super().__init__(vocab_file, never_split=[\"@@START@@\", \"@@END@@\", \"@@start@@\", \"@@end@@\"], **kwargs)\n",
    "    \n",
    "        self.tokenized_list = []\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        token_list = text.split()\n",
    "        split_tokens = []\n",
    "        tokenized_list = []\n",
    "        \n",
    "        if self.do_basic_tokenize:\n",
    "            for token in token_list:\n",
    "\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                    tokenized_list.append(1)\n",
    "                else:\n",
    "                    word_tokenized = self.wordpiece_tokenizer.tokenize(token)\n",
    "                    split_tokens += word_tokenized\n",
    "                    tokenized_list.append(len(word_tokenized))\n",
    "\n",
    "        self.tokenized_list.append(tokenized_list)\n",
    "        return split_tokens\n",
    "    \n",
    "def get_cumm(vals):\n",
    "    cumm = 0\n",
    "    res = [0] ## len of res is 1 more than vals, with an initial 0\n",
    "    for val in vals:\n",
    "        cumm += val\n",
    "        res.append(cumm)\n",
    "    return res\n",
    "\n",
    "def my_tokenize_function(data):\n",
    "    \n",
    "    ## tokenize\n",
    "    my_tokenizer.tokenized_list = []\n",
    "    result = my_tokenizer([\" \".join(eg) for eg in data[\"tokens\"]])\n",
    "    if my_tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    ## save word to token mapping\n",
    "    ## 3, 1, 1 means the first word got divided into 3 tokens, the next into 1, and the next into 1 again\n",
    "    result[\"word_tokens\"] = my_tokenizer.tokenized_list\n",
    "    result[\"cummulative_word_tokens\"] = [get_cumm(x) for x in result[\"word_tokens\"]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_kg_embedding_batched(data):\n",
    "    \n",
    "    ## store a masking array that says whether or not an item has kg embedding\n",
    "    \"\"\"\n",
    "    When you specify batched=True the function receives a dictionary with the fields of the dataset, \n",
    "    but each value is now a list of values, and not just a single value. \n",
    "    \"\"\"\n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    annotations_list = data['annotations']\n",
    "    cummulative_word_tokens_list = data[\"cummulative_word_tokens\"]\n",
    "    \n",
    "    batch_size = len(input_ids_list)\n",
    "    embed_list = [] ## len will be batch_size\n",
    "    embed_mask = []\n",
    "    embed_mask_qid = []\n",
    "    \n",
    "    #add by Edward, the index of qid\n",
    "    embed_mask_index = []\n",
    "    \n",
    "    allc = 0\n",
    "    cc = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        input_ids = input_ids_list[i]\n",
    "        annotations = annotations_list[i]\n",
    "        \n",
    "        ## Replace zeros with random numbers if required\n",
    "        embeds = np.zeros((len(input_ids), 200)) ## CLS, SEP will have np.zeros, like unknown words\n",
    "        mask = [0]*len(input_ids)\n",
    "        mask_qid = ['0']*len(input_ids)\n",
    "        \n",
    "        #add by Edward\n",
    "        mask_index = [-100]*len(input_ids)\n",
    "        \n",
    "        \n",
    "        for annot in annotations:\n",
    "            start_ix, end_ix = annot['span']\n",
    "            start = cummulative_word_tokens_list[i][start_ix] + 1\n",
    "            end = cummulative_word_tokens_list[i][end_ix] + 1\n",
    "            \n",
    "            qid = annot['id']\n",
    "            \n",
    "            #add by Edward\n",
    "            index_list = qids_wktxt[qids_wktxt[\"id\"]==qid].index.tolist()\n",
    "            allc += 1\n",
    "            if len(index_list) == 0:\n",
    "                qid_index = -100\n",
    "\n",
    "            else:\n",
    "                qid_index = index_list[0]\n",
    "                cc+=1\n",
    "            \n",
    "            df = embeds_wktxt[embeds_wktxt['id']==qid]\n",
    "            if len(df)>0:\n",
    "                embeds[start:end] = np.tile(df.iloc[0,1:].values.reshape((1,200)),(end-start, 1))\n",
    "                mask[start:end] = [1]*(end-start)\n",
    "                mask_qid[start:end] = [qid]*(end-start)\n",
    "                \n",
    "                #add by Edward\n",
    "                mask_index[start:end] = [qid_index]*(end-start)\n",
    "                \n",
    "                \n",
    "        embed_mask.append(mask)\n",
    "        embed_mask_qid.append(mask_qid)\n",
    "        embed_list.append(embeds)\n",
    "        \n",
    "        #add by Edward\n",
    "        embed_mask_index.append(mask_index)\n",
    "\n",
    "    \n",
    "    print(cc/allc)\n",
    "    print(cc)\n",
    "    print(allc)\n",
    "    return {\n",
    "        \"kg_embedding\": embed_list, \n",
    "        \"kg_embedding_mask\": embed_mask,\n",
    "        \"kg_embedding_mask_qid\": embed_mask_qid,\n",
    "        \"kg_embedding_mask_index\": embed_mask_index\n",
    "    }\n",
    "\n",
    "def filter_text_batched(data):\n",
    "    \n",
    "    new_data = {k:[] for k in data}\n",
    "    \n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    \n",
    "    ## remove [UNK] == 100 \n",
    "    indices_list = [[i for i,input_id in enumerate(input_ids) if input_id!=100]\n",
    "                        for input_ids in input_ids_list]\n",
    "    \n",
    "    for k in data:\n",
    "        for indices, data_list in zip(indices_list, data[k]):\n",
    "            new_data[k].append([data_list[ind] for ind in indices])\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def truncate_data(data):\n",
    "    maxlength = my_tokenizer.max_model_input_sizes[bert_model_name]\n",
    "\n",
    "    ## truncate to maxlength\n",
    "    for k in data:\n",
    "        data[k] = [x[:maxlength] for x in data[k]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Create a new labels column\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ae3812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerModified'.\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = BertTokenizerModified.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc596d6",
   "metadata": {},
   "source": [
    "# Data processing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f538eaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linked Wikitext dataset\n",
    "dataset_file = \"concat_dataset_v2\"\n",
    "\n",
    "\n",
    "# final_dataset = wikitest2_dataset.map(my_tokenize_function, batched=True)\\\n",
    "#                           .map(get_kg_embedding_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "#                           .remove_columns(['title', 'tokens', 'annotations', 'word_tokens', 'cummulative_word_tokens'])\\\n",
    "#                           .map(filter_text_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "#                           .map(group_texts, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "\n",
    "# final_dataset.save_to_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16d1ed4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-90b20df1b3afab02\n",
      "Found cached dataset json (/home/xiangru/.cache/huggingface/datasets/json/default-90b20df1b3afab02/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956f0886a3344e27a256c651095308db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Wikidata fact dataset\n",
    "synthetic_data = \"generate_test_data/sythetic_dataset.jsonl\"\n",
    "synthetic_dataset = load_dataset(\"json\", data_files={\"synthetic\": synthetic_data})\n",
    "\n",
    "dataset_file = \"tokenized_synthetic_dataset_1\"\n",
    "\n",
    "# tokenized_synthetic_dataset = synthetic_dataset.map(my_tokenize_function, batched=True)\\\n",
    "#                           .map(get_kg_embedding_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "#                           .remove_columns(['title', 'tokens', 'annotations', 'word_tokens', 'cummulative_word_tokens'])\\\n",
    "\n",
    "# tokenized_synthetic_dataset.save_to_disk(dataset_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fae5e844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at concat_dataset_v2/valid/cache-f6dda4b0ee1674f7.arrow and concat_dataset_v2/valid/cache-0fc4692bf4cddc18.arrow\n"
     ]
    }
   ],
   "source": [
    "## Load the saved tokenized dataset\n",
    "dataset_file = \"concat_dataset_v2\"\n",
    "final_dataset = load_from_disk(dataset_file)\n",
    "\n",
    "dataset_file = \"tokenized_synthetic_dataset_1\"\n",
    "tokenized_synthetic_dataset = load_from_disk(dataset_file)\n",
    "\n",
    "\n",
    "# # To test model with smaller sample dataset\n",
    "train_size = 100\n",
    "test_size = 300\n",
    "downsampled_dataset = final_dataset[\"valid\"].train_test_split(train_size=train_size, test_size=test_size, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7334d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fa9e0",
   "metadata": {},
   "source": [
    "## Create Data Collator for Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2314fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Create Data Collator for Masking\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from collections.abc import Mapping    \n",
    "    \n",
    "# NEW FOR EXTRA TEST    \n",
    "class CustomDataCollator(DataCollatorForWholeWordMask):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "            attention_mask = [e[\"attention_mask\"] for e in examples]\n",
    "            kg_embedding_mask = [e[\"kg_embedding_mask\"] for e in examples]\n",
    "            kg_embedding = [e[\"kg_embedding\"] for e in examples]\n",
    "            kg_embedding_mask_qid = [e[\"kg_embedding_mask_qid\"] for e in examples]\n",
    "            kg_embedding_mask_index = [e[\"kg_embedding_mask_index\"] for e in examples]\n",
    "        else:\n",
    "            raise Exception(\"Dataset needs to be in dictionary format\")\n",
    " \n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        #mask_labels = kg_embedding_mask\n",
    "        batch_mask = _torch_collate_batch(kg_embedding_mask, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)        \n",
    "        # Edward: Aisha avoided taking tokens that corresponds to entity into the MLM task, why?\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        \n",
    "        # by Edward\n",
    "        batch_attention_mask = _torch_collate_batch(attention_mask, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    " \n",
    "        batch_size = len(kg_embedding)\n",
    "        token_length = len(inputs[0])\n",
    "        embedding_size = len(kg_embedding[0][0])\n",
    " \n",
    "        kg_embedding = [kg_embds+[[0]*embedding_size]*(token_length-len(kg_embds)) for kg_embds in kg_embedding]\n",
    "    \n",
    "        #by Edward\n",
    "        batch_mask_index = _torch_collate_batch(kg_embedding_mask_index, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "        return {\n",
    "                \"input_ids\": inputs, \n",
    "                \"attention_mask\" : batch_attention_mask,\n",
    "                \"labels\": labels, \n",
    "                \"kg_embedding\":kg_embedding, \n",
    "                \"kg_embedding_mask\": batch_mask,\n",
    "                \"kg_embedding_mask_qid\": [qid+['0']*(token_length-len(qid)) for qid in kg_embedding_mask_qid],\n",
    "                \"kg_embedding_mask_index\":batch_mask_index,\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da62f9be",
   "metadata": {},
   "source": [
    "# Model trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e71372",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "982bdc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import json\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"Implementation of the gelu activation function.\n",
    "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
    "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "def swish(x):\n",
    "    return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish, \"GELU\":GELUActivation()}\n",
    "\n",
    "\n",
    "class BertConfig(object):\n",
    "    \"\"\"Configuration class to store the configuration of a `BertModel`.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size_or_config_json_file,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_act=\"GELU\",\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2,\n",
    "                 initializer_range=0.02,\n",
    "                 layer_types=None,\n",
    "                 hidden_size_ent=200,\n",
    "                 base_model_name=\"distilbert-base-uncased\",\n",
    "                 kg_vocab_size=46685,\n",
    "                 kg_num_attention_head=4):\n",
    "        \"\"\"Constructs BertConfig.\n",
    "        Args:\n",
    "            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n",
    "            hidden_size: Size of the encoder layers and the pooler layer.\n",
    "            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "            num_attention_heads: Number of attention heads for each attention layer in\n",
    "                the Transformer encoder.\n",
    "            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
    "                layer in the Transformer encoder.\n",
    "            hidden_act: The non-linear activation function (function or string) in the\n",
    "                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n",
    "            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n",
    "                layers in the embeddings, encoder, and pooler.\n",
    "            attention_probs_dropout_prob: The dropout ratio for the attention\n",
    "                probabilities.\n",
    "            max_position_embeddings: The maximum sequence length that this model might\n",
    "                ever be used with. Typically set this to something large just in case\n",
    "                (e.g., 512 or 1024 or 2048).\n",
    "            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
    "                `BertModel`.\n",
    "            initializer_range: The sttdev of the truncated_normal_initializer for\n",
    "                initializing all weight matrices.\n",
    "        \"\"\"\n",
    "        if isinstance(vocab_size_or_config_json_file, str):\n",
    "            with open(vocab_size_or_config_json_file, \"r\", encoding='utf-8') as reader:\n",
    "                json_config = json.loads(reader.read())\n",
    "            for key, value in json_config.items():\n",
    "                self.__dict__[key] = value\n",
    "        elif isinstance(vocab_size_or_config_json_file, int):\n",
    "            self.vocab_size = vocab_size_or_config_json_file\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_hidden_layers = num_hidden_layers\n",
    "            self.num_attention_heads = num_attention_heads\n",
    "            self.hidden_act = hidden_act\n",
    "            self.intermediate_size = intermediate_size\n",
    "            self.hidden_dropout_prob = hidden_dropout_prob\n",
    "            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "            self.max_position_embeddings = max_position_embeddings\n",
    "            self.type_vocab_size = type_vocab_size\n",
    "            self.initializer_range = initializer_range\n",
    "            self.layer_types = layer_types\n",
    "            self.hidden_size_ent = hidden_size_ent\n",
    "            self.base_model_name = base_model_name\n",
    "            self.kg_vocab_size = kg_vocab_size\n",
    "            self.kg_num_attention_head = kg_num_attention_head\n",
    "        else:\n",
    "            raise ValueError(\"First argument must be either a vocabulary size (int)\"\n",
    "                             \"or the path to a pretrained model config file (str)\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, json_object):\n",
    "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "        config = BertConfig(vocab_size_or_config_json_file=-1)\n",
    "        for key, value in json_object.items():\n",
    "            config.__dict__[key] = value\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file):\n",
    "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "        with open(json_file, \"r\", encoding='utf-8') as reader:\n",
    "            text = reader.read()\n",
    "        return cls.from_dict(json.loads(text))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.to_json_string())\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
    "    \n",
    "    def to_json_file(self,path):\n",
    "        with open(path, \"w\") as outfile:\n",
    "            outfile.write(self.to_json_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53516a21",
   "metadata": {},
   "source": [
    "## Abstract model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e751243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTrainedBertModel(nn.Module):\n",
    "    \"\"\" An abstract class to handle weights initialization and\n",
    "        a simple interface for dowloading and loading pretrained models.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(PreTrainedBertModel, self).__init__()\n",
    "        if not isinstance(config, BertConfig):\n",
    "            raise ValueError(\n",
    "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
    "                \"To create a model from a Google pretrained model use \"\n",
    "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
    "                    self.__class__.__name__, self.__class__.__name__\n",
    "                ))\n",
    "        self.config = config\n",
    "\n",
    "    def init_bert_weights(self, module):\n",
    "        \"\"\" Initialize the weights.\n",
    "        \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, BertLayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a840d180",
   "metadata": {},
   "source": [
    "## Integration module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9263f3",
   "metadata": {},
   "source": [
    "### Attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5fefbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super(BertLayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07663085",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        return context_layer\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce66a238",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertAttention, self).__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "        config_ent = copy.deepcopy(config)\n",
    "        config_ent.hidden_size = config.hidden_size_ent\n",
    "        config_ent.num_attention_heads = config.kg_num_attention_head\n",
    "\n",
    "        self.self_ent = BertSelfAttention(config_ent)\n",
    "        self.output_ent = BertSelfOutput(config_ent)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor, attention_mask, input_tensor_ent, attention_mask_ent):\n",
    "        self_output = self.self(input_tensor, attention_mask)\n",
    "        self_output_ent = self.self_ent(input_tensor_ent, attention_mask_ent)\n",
    "        attention_output = self.output(self_output, input_tensor)\n",
    "        attention_output_ent = self.output_ent(self_output_ent, input_tensor_ent)\n",
    "        return attention_output, attention_output_ent\n",
    "    \n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertIntermediate, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.dense_ent = nn.Linear(config.hidden_size_ent, config.intermediate_size)\n",
    "\n",
    "        #self.dense_1 = nn.Linear(config.hidden_size, 200)\n",
    "        #self.dense_1_ent = nn.Linear(100, 200)\n",
    "\n",
    "#         self.intermediate_act_fn = ACT2FN[config.hidden_act] \\\n",
    "#             if isinstance(config.hidden_act, str) else config.hidden_act\n",
    "\n",
    "        self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "\n",
    "    def forward(self, hidden_states, hidden_states_ent):\n",
    "        hidden_states_ = self.dense(hidden_states)\n",
    "        hidden_states_ent_ = self.dense_ent(hidden_states_ent)\n",
    "\n",
    "        #hidden_states_1 = self.dense_1(hidden_states)\n",
    "        #hidden_states_ent_1 = self.dense_1_ent(hidden_states_ent)\n",
    "\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states_+hidden_states_ent_)\n",
    "        #hidden_states_ent = self.intermediate_act_fn(hidden_states_1+hidden_states_ent_1)\n",
    "\n",
    "        return hidden_states#, hidden_states_ent\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertOutput, self).__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.dense_ent = nn.Linear(config.intermediate_size, config.hidden_size_ent)\n",
    "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.LayerNorm_ent = BertLayerNorm(config.hidden_size_ent, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states_, input_tensor, input_tensor_ent):\n",
    "        hidden_states = self.dense(hidden_states_)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "\n",
    "        hidden_states_ent = self.dense_ent(hidden_states_)\n",
    "        hidden_states_ent = self.dropout(hidden_states_ent)\n",
    "        hidden_states_ent = self.LayerNorm_ent(hidden_states_ent + input_tensor_ent)\n",
    "\n",
    "        return hidden_states, hidden_states_ent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2923787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertLayer, self).__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask, hidden_states_ent, attention_mask_ent, ent_mask):\n",
    "        attention_output, attention_output_ent = self.attention(hidden_states, attention_mask, hidden_states_ent, attention_mask_ent)\n",
    "        \n",
    "        \n",
    "#         print(ent_mask[0])\n",
    "        \n",
    "#         for i in range(10):\n",
    "#             print(attention_output_ent[0][i])\n",
    "        \n",
    "\n",
    "        \n",
    "        attention_output_ent = attention_output_ent * ent_mask.unsqueeze(2)\n",
    "        \n",
    "#         for i in range(10):\n",
    "#             print(attention_output_ent[0][i])\n",
    "\n",
    "        \n",
    "        intermediate_output = self.intermediate(attention_output, attention_output_ent)\n",
    "        layer_output, layer_output_ent = self.output(intermediate_output, attention_output, attention_output_ent)\n",
    "        # layer_output_ent = layer_output_ent * ent_mask\n",
    "        return layer_output, layer_output_ent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986d2a1",
   "metadata": {},
   "source": [
    "### Main module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cb08f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordItgtor(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(WordItgtor, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layer = BertLayer(config)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self,  hidden_states, attention_mask, hidden_states_ent, \n",
    "                ent_mask,output_all_encoded_layers=False):        \n",
    "\n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "        extended_ent_mask = ent_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        extended_ent_mask = extended_ent_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_ent_mask = (1.0 - extended_ent_mask) * -10000.0\n",
    "        \n",
    "        res_state, res_state_ent = self.layer(hidden_states,\n",
    "                                      extended_attention_mask,\n",
    "                                      hidden_states_ent,\n",
    "                                      extended_ent_mask,\n",
    "                                      ent_mask)\n",
    "#         sequence_output = res_state[-1]\n",
    "#         pooled_output = self.pooler(sequence_output)\n",
    "\n",
    "        encoded_layers = res_state\n",
    "        return encoded_layers  #, pooled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e272b5e",
   "metadata": {},
   "source": [
    "## Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce62c0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#class BERTModified(nn.Module): #PreTrainedModel\n",
    "class BERTModified(PreTrainedBertModel): #\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__(config) #For PreTrainedModel\n",
    "        #super().__init__() ## for nn.Module\n",
    "        \n",
    "        self.base_model = AutoModel.from_pretrained(self.config.base_model_name)\n",
    "        self.config = config\n",
    "        self.kg_size = self.config.kg_vocab_size\n",
    "                \n",
    "        self.activation = GELUActivation() # for distilbert\n",
    "        self.vocab_transform = nn.Linear(self.config.hidden_size, self.config.hidden_size)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(self.config.hidden_size, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(self.config.hidden_size, self.config.vocab_size)\n",
    "        \n",
    "        self.kg_projector = nn.Linear(self.config.hidden_size, self.kg_size)\n",
    "        \n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ## set to eval\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        ## freeze model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        ## initialization of integrator\n",
    "#         self.itgt = WordItgtor(self.config.hidden_size,self.config.hidden_size_ent)\n",
    "        self.itgt = WordItgtor(self.config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kg_embedding = None,           ## given\n",
    "        kg_embedding_mask = None,      ## given\n",
    "        kg_embedding_mask_qid = None,      ## given\n",
    "        kg_embedding_mask_index = None,\n",
    "        input_ids = None,              ## given\n",
    "        attention_mask = None,         ## given\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,                 ## given\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,):\n",
    "        \n",
    "        base_model_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "\n",
    "        \n",
    "        ## Get LM embedding\n",
    "        hidden_states = base_model_output[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "\n",
    "        \n",
    "        ## Use hidden_states and kg_embedding and perform INTEGRATION\n",
    "        kg_embedding = torch.tensor(kg_embedding).to(device='cuda:0')\n",
    "        kg_embedding_mask_index = torch.tensor(kg_embedding_mask_index).to(device='cuda:0')\n",
    "        kg_embedding_mask = torch.tensor(kg_embedding_mask).to(device='cuda:0')\n",
    "        \n",
    "        #print(attention_mask)\n",
    "        \n",
    "        attention_mask = torch.tensor(attention_mask).to(device='cuda:0')\n",
    "        \n",
    "        prediction_logits = self.itgt(hidden_states,attention_mask, kg_embedding, kg_embedding_mask)\n",
    "#         prediction_logits = self.itgt(hidden_states, kg_embedding)\n",
    "#         prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "    \n",
    "        lm_prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        kg_prediction_logits = self.kg_projector(prediction_logits)  # (bs, seq_length, kg_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        kg_loss = None\n",
    "        \n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(lm_prediction_logits.view(-1, lm_prediction_logits.size(-1)), labels.view(-1))\n",
    "        if kg_embedding_mask_index is not None:\n",
    "            kg_loss = self.mlm_loss_fct(kg_prediction_logits.view(-1, kg_prediction_logits.size(-1)), kg_embedding_mask_index.view(-1))\n",
    "\n",
    "        total_loss = mlm_loss + kg_loss\n",
    "        \n",
    "        return MaskedLMOutput(\n",
    "            loss=total_loss,\n",
    "            logits=lm_prediction_logits,\n",
    "            hidden_states=base_model_output.hidden_states,\n",
    "            attentions=base_model_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5aecc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "166a7e57",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197447bd",
   "metadata": {},
   "source": [
    "1. The KIM designed in this study is BERTModified. \n",
    "2. The basiline LM-Raw is BERTModified_LMRaw.\n",
    "3. The basiline KG-Raw is BERTModified_KGRaw.\n",
    "4. The alternative integration module Alt-KIM is BERTModified_alt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ac2e8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Our KIM\"  # or LM-Raw, KG-Raw, alt-KIM\n",
    "\n",
    "model_dict ={\"Our KIM\":BERTModified, \"LM-Raw\":BERTModified_LMRaw, \"KG-Raw\":BERTModified_KGRaw,\"alt-KIM\":BERTModified_alt}\n",
    "\n",
    "#base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "\n",
    "a = BertConfig(30522)\n",
    "a.to_json_file('sample.json')\n",
    "\n",
    "config = BertConfig.from_json_file('sample.json')\n",
    "\n",
    "# choose the model needsto be trained \n",
    "# model = model_dict[model_name](bert_model_name = bert_model_name,\n",
    "#                                   base_model = base_model,\n",
    "#                                   config = config, kg_size =46685)\n",
    "model = model_dict[model_name](config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d2e30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data_collator\n",
    "data_collator = CustomDataCollator(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b0cc4f",
   "metadata": {},
   "source": [
    "# Evaluation metrics and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25c77502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# metrics = evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds=None, logits=None, labels=None):\n",
    "    \n",
    "    # We should have either `eval_preds` or both `logits` and `labels`\n",
    "    if eval_preds:\n",
    "        logits, labels = eval_preds\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    ## Flatten values\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f868c8e",
   "metadata": {},
   "source": [
    "# Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a8d2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(final_dataset['train']) // batch_size #len(final_dataset['train']) // batch_size\n",
    "model_name = \"BERTModified\"\n",
    "output_dir = f\"{model_name}-finetuned-wikitext-test\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "#     fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=50,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"loss\",#metric_name,\n",
    "#     greater_is_better = False,\n",
    "    logging_dir='logs',\n",
    "    report_to=\"wandb\",\n",
    "#     no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ed9ae",
   "metadata": {},
   "source": [
    "metric_for_best_model (str, optional) — Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\". Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "\n",
    "If you set this value, greater_is_better will default to True. Don’t forget to set it to False if your metric is better when lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd221993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/KGNLP/KG-LM-Integration/BERTModified-finetuned-wikitext-test is already a clone of https://huggingface.co/HideOnBush/BERTModified-finetuned-wikitext-test. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "#     train_dataset=final_dataset[\"train\"],\n",
    "#     eval_dataset=final_dataset[\"valid\"],\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c14e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset=downsampled_dataset[\"train\"]\n",
    "# eval_dataset=downsampled_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5f89c",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12cfd7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from ./BERTModified-finetuned-wikitext-test/checkpoint-175280.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 17528\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 219100\n",
      "  Number of trainable parameters = 68445855\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 40\n",
      "  Continuing training from global step 175280\n",
      "  Will skip the first 40 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428651a4342448aa83dc4331e2515b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medwardjian\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xiangru/KGNLP/KG-LM-Integration/wandb/run-20230308_034538-2utw4moe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/edwardjian/huggingface/runs/2utw4moe\" target=\"_blank\">BERTModified-finetuned-wikitext-test</a></strong> to <a href=\"https://wandb.ai/edwardjian/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='219100' max='219100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [219100/219100 2:18:51, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>16.566595</td>\n",
       "      <td>0.338112</td>\n",
       "      <td>0.338112</td>\n",
       "      <td>0.338112</td>\n",
       "      <td>0.338112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.794700</td>\n",
       "      <td>16.591980</td>\n",
       "      <td>0.329290</td>\n",
       "      <td>0.329290</td>\n",
       "      <td>0.329290</td>\n",
       "      <td>0.329290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.759000</td>\n",
       "      <td>16.609325</td>\n",
       "      <td>0.333480</td>\n",
       "      <td>0.333480</td>\n",
       "      <td>0.333480</td>\n",
       "      <td>0.333480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.736100</td>\n",
       "      <td>16.686855</td>\n",
       "      <td>0.334142</td>\n",
       "      <td>0.334142</td>\n",
       "      <td>0.334142</td>\n",
       "      <td>0.334142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.709700</td>\n",
       "      <td>16.696791</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>0.336127</td>\n",
       "      <td>0.336127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>16.714911</td>\n",
       "      <td>0.334804</td>\n",
       "      <td>0.334804</td>\n",
       "      <td>0.334804</td>\n",
       "      <td>0.334804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.672300</td>\n",
       "      <td>16.600849</td>\n",
       "      <td>0.338553</td>\n",
       "      <td>0.338553</td>\n",
       "      <td>0.338553</td>\n",
       "      <td>0.338553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.661600</td>\n",
       "      <td>16.550951</td>\n",
       "      <td>0.339876</td>\n",
       "      <td>0.339876</td>\n",
       "      <td>0.339876</td>\n",
       "      <td>0.339876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.648700</td>\n",
       "      <td>16.684456</td>\n",
       "      <td>0.333701</td>\n",
       "      <td>0.333701</td>\n",
       "      <td>0.333701</td>\n",
       "      <td>0.333701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.637900</td>\n",
       "      <td>16.619131</td>\n",
       "      <td>0.338774</td>\n",
       "      <td>0.338774</td>\n",
       "      <td>0.338774</td>\n",
       "      <td>0.338774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-179662\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-184044\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-188426\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-192808\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-210336\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-214718\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-finetuned-wikitext-test/checkpoint-219100\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=219100, training_loss=0.14265076324309675, metrics={'train_runtime': 8335.1575, 'train_samples_per_second': 105.145, 'train_steps_per_second': 26.286, 'total_flos': 0.0, 'train_loss': 0.14265076324309675, 'epoch': 50.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train()\n",
    "\n",
    "#training from a checkpoint\n",
    "trainer.train(\"./BERTModified-finetuned-wikitext-test/checkpoint-175280\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b4ed6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 300\n",
      "  Batch size = 4\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 15699980.03\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 16.569169998168945,\n",
       " 'eval_precision': 0.30966034406704895,\n",
       " 'eval_recall': 0.30966034406704895,\n",
       " 'eval_f1': 0.30966034406704895,\n",
       " 'eval_accuracy': 0.30966034406704895,\n",
       " 'eval_runtime': 16.5226,\n",
       " 'eval_samples_per_second': 18.157,\n",
       " 'eval_steps_per_second': 4.539,\n",
       " 'epoch': 25.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# basic evluation on perplexity \n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") #21596 for 1 epoch\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd9be2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d68f90",
   "metadata": {},
   "source": [
    "## 1.LM-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0283664",
   "metadata": {},
   "source": [
    "### 1.1 Linked Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ebb2e81f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 2069\n",
      "  Batch size = 4\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/ipykernel_launcher.py:71: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19487/920390983.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2869\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2870\u001b[0m         output = eval_loop(\n\u001b[0;32m-> 2871\u001b[0;31m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2872\u001b[0m         )\n\u001b[1;32m   2873\u001b[0m         \u001b[0mtotal_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_batch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2962\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2963\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2964\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2965\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2966\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/utils/data/_utils/pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[0;34m(data, device)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/abc.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;34m\"\"\"Override for isinstance(instance, cls).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_abc_instancecheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(final_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f585e",
   "metadata": {},
   "source": [
    "[About micro, macro, weighted precision, recall, f1 for multiclass labels](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)\n",
    "\n",
    "The following always holds true for the micro-F1 case:\n",
    "\n",
    "`micro-F1 = micro-precision = micro-recall = accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de842903",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show the result\n",
    "ompute_metrics(logits = predictions.predictions, labels = predictions.label_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678c741",
   "metadata": {},
   "source": [
    "### 1.2 Wikidata facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f756533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_synthetic_dataset[\"synthetic\"])\n",
    "\n",
    "compute_metrics(logits = predictions.predictions, labels = predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c93337",
   "metadata": {},
   "source": [
    "## 2.KG-accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1729b7f3",
   "metadata": {},
   "source": [
    "### 2.1 Linked Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a51cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model_name = \"Our KIM\"\n",
    "model_dict_kg ={\"Our KIM\":BERTModified_KG, \"KG-Raw\":BERTModified_KGRaw,\"alt-KIM\":BERTModified_altKG}\n",
    "\n",
    "base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "# choose the model needsto be trained \n",
    "model = model_dict_kg[model_name](bert_model_name = bert_model_name,\n",
    "                                  base_model = base_model,\n",
    "                                  config = base_model.config,kg_size =46685)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpoint = torch.load(\"./\" + output_dir + \"/pytorch_model.bin\")\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74eff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_dataset[\"train\"],\n",
    "    eval_dataset=final_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = trainer.predict(final_dataset[\"test\"]) \n",
    "\n",
    "labels = np.array(final_dataset[\"test\"][\"kg_embedding_mask_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbbe8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# show the result\n",
    "compute_metrics(logits = predictions.predictions, labels =labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be989d04",
   "metadata": {},
   "source": [
    "### 2.2 Wikidata facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a73418",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(tokenized_synthetic_dataset[\"synthetic\"])\n",
    "# true label of entitles\n",
    "labels = np.array(tokenized_synthetic_dataset[\"synthetic\"][\"kg_embedding_mask_index\"])\n",
    "\n",
    "# show the result\n",
    "compute_metrics(logits = predictions.predictions, labels =labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6883f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
