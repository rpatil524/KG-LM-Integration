{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f5d47c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415ab001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/xiangru/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers.data.data_collator import _torch_collate_batch\n",
    "import evaluate\n",
    "\n",
    "import wandb\n",
    "# wandb.init(project=\"kg-lm-integration\", entity=\"tanny411\")\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#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\n",
    "emb_tsv_file = \"wikidata_translation_v1.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d2eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "# but doing so is dangerous -- the standard limit is a little conservative, but Python stackframes can be quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed5a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"distilbert-base-uncased\" ##\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a6ac",
   "metadata": {},
   "source": [
    "# Initialize Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8ed2b",
   "metadata": {},
   "source": [
    "Download:\n",
    "- embeds_wktxt.csv\n",
    "- [linked-wikitext-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/) and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41ccbd",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495d87e",
   "metadata": {},
   "source": [
    "- `tokens` are the given list of tokens from wikitext2\n",
    "- `input_ids` are what come from tokenization, they divide certain words into multiple pieces, and each sentence has a CLS and a SEP\n",
    "- `word_tokens` is the length of `tokens`. For each token in `token`, it mentions how many sub-words it was divided into due to word piece tokenization\n",
    "- `cummulative_word_tokens` is a cummulative sum of `word_tokens`, with an extra 0 in the beginning\n",
    "\n",
    "##### Process\n",
    "index of a token in `token` can be found in `input_ids` by `cummulative_word_tokens`. if `ix` is the index of a word in `token`, its beginning index in `input_ids` is `cummulative_word_tokens[ix] + 1`, the +1 is because `input_ids` has a CLS in the beginning. `token[ix]` spans from `input_ids[cummulative_word_tokens[ix] + 1]` to `input_ids[cummulative_word_tokens[ix+1] + 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efb55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0facda0eb7ba06ee\n",
      "Found cached dataset json (/home/xiangru/.cache/huggingface/datasets/json/default-0facda0eb7ba06ee/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2081521624d24eeea48985fe82102dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeds_wktxt = pd.read_csv(\"embeds_wktxt.csv\")\n",
    "qids_wktxt = pd.read_csv(\"qids_wktxt2.csv\")\n",
    "\n",
    "linked_wikitext_2 = \"linked-wikitext-2/\"\n",
    "train = linked_wikitext_2+\"train.jsonl\"\n",
    "valid = linked_wikitext_2+\"valid.jsonl\"\n",
    "test = linked_wikitext_2+\"test.jsonl\"\n",
    "\n",
    "data_files = {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "wikitest2_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "chunk_size = 128\n",
    "\n",
    "class BertTokenizerModified(DistilBertTokenizer): #BertTokenizer\n",
    "    def __init__(self,vocab_file,**kwargs):\n",
    "        \n",
    "        super().__init__(vocab_file, never_split=[\"@@START@@\", \"@@END@@\", \"@@start@@\", \"@@end@@\"], **kwargs)\n",
    "    \n",
    "        self.tokenized_list = []\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        token_list = text.split()\n",
    "        split_tokens = []\n",
    "        tokenized_list = []\n",
    "        \n",
    "        if self.do_basic_tokenize:\n",
    "            for token in token_list:\n",
    "\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                    tokenized_list.append(1)\n",
    "                else:\n",
    "                    word_tokenized = self.wordpiece_tokenizer.tokenize(token)\n",
    "                    split_tokens += word_tokenized\n",
    "                    tokenized_list.append(len(word_tokenized))\n",
    "\n",
    "        self.tokenized_list.append(tokenized_list)\n",
    "        return split_tokens\n",
    "    \n",
    "def get_cumm(vals):\n",
    "    cumm = 0\n",
    "    res = [0] ## len of res is 1 more than vals, with an initial 0\n",
    "    for val in vals:\n",
    "        cumm += val\n",
    "        res.append(cumm)\n",
    "    return res\n",
    "\n",
    "def my_tokenize_function(data):\n",
    "    \n",
    "    ## tokenize\n",
    "    my_tokenizer.tokenized_list = []\n",
    "    result = my_tokenizer([\" \".join(eg) for eg in data[\"tokens\"]])\n",
    "    if my_tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    ## save word to token mapping\n",
    "    ## 3, 1, 1 means the first word got divided into 3 tokens, the next into 1, and the next into 1 again\n",
    "    result[\"word_tokens\"] = my_tokenizer.tokenized_list\n",
    "    result[\"cummulative_word_tokens\"] = [get_cumm(x) for x in result[\"word_tokens\"]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_kg_embedding_batched(data):\n",
    "    \n",
    "    ## store a masking array that says whether or not an item has kg embedding\n",
    "    \"\"\"\n",
    "    When you specify batched=True the function receives a dictionary with the fields of the dataset, \n",
    "    but each value is now a list of values, and not just a single value. \n",
    "    \"\"\"\n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    annotations_list = data['annotations']\n",
    "    cummulative_word_tokens_list = data[\"cummulative_word_tokens\"]\n",
    "    \n",
    "    batch_size = len(input_ids_list)\n",
    "    embed_list = [] ## len will be batch_size\n",
    "    embed_mask = []\n",
    "    embed_mask_qid = []\n",
    "    \n",
    "    #add by Edward, the index of qid\n",
    "    embed_mask_index = []\n",
    "    \n",
    "    allc = 0\n",
    "    cc = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        input_ids = input_ids_list[i]\n",
    "        annotations = annotations_list[i]\n",
    "        \n",
    "        ## Replace zeros with random numbers if required\n",
    "        embeds = np.zeros((len(input_ids), 200)) ## CLS, SEP will have np.zeros, like unknown words\n",
    "        mask = [0]*len(input_ids)\n",
    "        mask_qid = ['0']*len(input_ids)\n",
    "        \n",
    "        #add by Edward\n",
    "        mask_index = [-100]*len(input_ids)\n",
    "        \n",
    "        \n",
    "        for annot in annotations:\n",
    "            start_ix, end_ix = annot['span']\n",
    "            start = cummulative_word_tokens_list[i][start_ix] + 1\n",
    "            end = cummulative_word_tokens_list[i][end_ix] + 1\n",
    "            \n",
    "            qid = annot['id']\n",
    "            \n",
    "            #add by Edward\n",
    "            index_list = qids_wktxt[qids_wktxt[\"id\"]==qid].index.tolist()\n",
    "            allc += 1\n",
    "            if len(index_list) == 0:\n",
    "                qid_index = -100\n",
    "\n",
    "            else:\n",
    "                qid_index = index_list[0]\n",
    "                cc+=1\n",
    "            \n",
    "            df = embeds_wktxt[embeds_wktxt['id']==qid]\n",
    "            if len(df)>0:\n",
    "                embeds[start:end] = np.tile(df.iloc[0,1:].values.reshape((1,200)),(end-start, 1))\n",
    "                mask[start:end] = [1]*(end-start)\n",
    "                mask_qid[start:end] = [qid]*(end-start)\n",
    "                \n",
    "                #add by Edward\n",
    "                mask_index[start:end] = [qid_index]*(end-start)\n",
    "                \n",
    "                \n",
    "        embed_mask.append(mask)\n",
    "        embed_mask_qid.append(mask_qid)\n",
    "        embed_list.append(embeds)\n",
    "        \n",
    "        #add by Edward\n",
    "        embed_mask_index.append(mask_index)\n",
    "\n",
    "    \n",
    "    print(cc/allc)\n",
    "    return {\n",
    "        \"kg_embedding\": embed_list, \n",
    "        \"kg_embedding_mask\": embed_mask,\n",
    "        \"kg_embedding_mask_qid\": embed_mask_qid,\n",
    "        \"kg_embedding_mask_index\": embed_mask_index\n",
    "    }\n",
    "\n",
    "def filter_text_batched(data):\n",
    "    \n",
    "    new_data = {k:[] for k in data}\n",
    "    \n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    \n",
    "    ## remove [UNK] == 100 \n",
    "    indices_list = [[i for i,input_id in enumerate(input_ids) if input_id!=100]\n",
    "                        for input_ids in input_ids_list]\n",
    "    \n",
    "    for k in data:\n",
    "        for indices, data_list in zip(indices_list, data[k]):\n",
    "            new_data[k].append([data_list[ind] for ind in indices])\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def truncate_data(data):\n",
    "    maxlength = my_tokenizer.max_model_input_sizes[bert_model_name]\n",
    "\n",
    "    ## truncate to maxlength\n",
    "    for k in data:\n",
    "        data[k] = [x[:maxlength] for x in data[k]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Create a new labels column\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ae3812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerModified'.\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = BertTokenizerModified.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2f2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"concat_dataset_v4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538eaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dataset = wikitest2_dataset.map(my_tokenize_function, batched=True)\\\n",
    "                          .map(get_kg_embedding_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .remove_columns(['title', 'tokens', 'annotations', 'word_tokens', 'cummulative_word_tokens'])\\\n",
    "                          .map(filter_text_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .map(group_texts, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "\n",
    "final_dataset.save_to_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fae5e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved tokenized dataset\n",
    "final_dataset = load_from_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b036f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 17528\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 1798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 2069\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2314fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Data Collator for Masking\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class CustomDataCollator(DataCollatorForWholeWordMask):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "            kg_embedding_mask = [e[\"kg_embedding_mask\"] for e in examples]\n",
    "            kg_embedding = [e[\"kg_embedding\"] for e in examples]\n",
    "            kg_embedding_mask_qid = [e[\"kg_embedding_mask_qid\"] for e in examples]\n",
    "            \n",
    "            #add by Edward\n",
    "            kg_embedding_mask_index = [e[\"kg_embedding_mask_index\"] for e in examples]\n",
    "        else:\n",
    "            raise Exception(\"Dataset needs to be in dictionary format\")\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = kg_embedding_mask\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        \n",
    "        return {\n",
    "                \"input_ids\": inputs, \n",
    "                \"labels\": labels, \n",
    "                \"kg_embedding\":kg_embedding, \n",
    "                \"kg_embedding_mask\":kg_embedding_mask,\n",
    "                \"kg_embedding_mask_qid\":kg_embedding_mask_qid,\n",
    "                \"kg_embedding_mask_index\":kg_embedding_mask_index,\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14e07f",
   "metadata": {},
   "source": [
    "# Integrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfc69fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordItgtor(nn.Module):\n",
    "    \n",
    "    # seems like the dimension of the language model embedding will be 768 while that for \n",
    "    # knowledge graph embedding is 200, the module is kind of built based on this fact that \n",
    "    # the former has larger dimension\n",
    "    def __init__(self, embed_dim_lm, embed_dim_kg):\n",
    "        super(WordItgtor, self).__init__()\n",
    "        \n",
    "        self.tt_embed_dim = embed_dim_lm, embed_dim_kg\n",
    "        \n",
    "        self.fc_kg = nn.Linear(embed_dim_kg,embed_dim_lm)\n",
    "        self.fc_lm = nn.Linear(embed_dim_lm,embed_dim_lm)\n",
    "        self.fc1 = nn.Linear(embed_dim_lm * 2, embed_dim_lm * 4)\n",
    "        self.fc2 = nn.Linear(embed_dim_lm * 4, embed_dim_lm * 2)\n",
    "        self.fc3 = nn.Linear(embed_dim_lm * 2, embed_dim_lm)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x_lm, x_kg, kg_mask=None):        \n",
    "        # x_lm_raw is the embedding of a single sentence from language model with size \n",
    "        # ((number of words) * embed_dim_lm)\n",
    "        \n",
    "        # x_kg is the embedding of a single sentence from knowledge graph with size \n",
    "        # ((number of words) * embed_dim_lm)\n",
    "        \n",
    "#         if kg_mask == 0:\n",
    "#             return x_lm\n",
    "        \n",
    "        x_kg = self.fc_kg(x_kg)\n",
    "        x_lm = self.fc_lm(x_lm)\n",
    "        \n",
    "        x = torch.cat((x_lm,x_kg),dim=-1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a7e57",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "682f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class BERTModified(nn.Module): #PreTrainedModel\n",
    "    def __init__(self, bert_model_name, base_model, config):\n",
    "        \n",
    "#         super().__init__(config) #For PreTrainedModel\n",
    "        super().__init__() ## for nn.Module\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.config = config\n",
    "        self.kg_size = qids_wktxt.shape[0]\n",
    "                \n",
    "        self.activation = GELUActivation() # for distilbert\n",
    "        self.vocab_transform = nn.Linear(self.config.dim, self.config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(self.config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "        \n",
    "        self.kg_projector = nn.Linear(self.config.dim, self.kg_size)\n",
    "        \n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ## set to eval\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        ## freeze model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        ## initialization of integrator\n",
    "        self.itgt = WordItgtor(self.config.dim,200)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kg_embedding = None,           ## given\n",
    "        kg_embedding_mask = None,      ## given\n",
    "        kg_embedding_mask_qid = None,      ## given\n",
    "        kg_embedding_mask_index = None,\n",
    "        input_ids = None,              ## given\n",
    "        attention_mask = None,         ## given\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,                 ## given\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,):\n",
    "        \n",
    "        base_model_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        ## Get LM embedding\n",
    "        hidden_states = base_model_output[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "\n",
    "        \n",
    "        ## TODO: Use hidden_states and kg_embedding and perform INTEGRATION\n",
    "#         prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        \n",
    "        \n",
    "        kg_embedding = torch.tensor(kg_embedding).to(device='cuda:0')\n",
    "        kg_embedding_mask_index = torch.tensor(kg_embedding_mask_index).to(device='cuda:0')\n",
    "        \n",
    "        prediction_logits = self.itgt(hidden_states, kg_embedding)\n",
    "        prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        lm_prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        kg_prediction_logits = self.kg_projector(prediction_logits)  # (bs, seq_length, kg_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        kg_loss = None\n",
    "        \n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(lm_prediction_logits.view(-1, lm_prediction_logits.size(-1)), labels.view(-1))\n",
    "        if kg_embedding_mask_index is not None:\n",
    "            kg_loss = self.mlm_loss_fct(kg_prediction_logits.view(-1, kg_prediction_logits.size(-1)), kg_embedding_mask_index.view(-1))\n",
    "\n",
    "        total_loss = mlm_loss + kg_loss\n",
    "        \n",
    "        return MaskedLMOutput(\n",
    "            loss=total_loss,\n",
    "            logits=lm_prediction_logits,\n",
    "            hidden_states=base_model_output.hidden_states,\n",
    "            attentions=base_model_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac2e8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "BERTModified_model = BERTModified(bert_model_name = bert_model_name,\n",
    "                                  base_model = base_model,\n",
    "                                  config = base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0373295a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_updated_checkp = \"checkpoint-65730\"\n",
    "checkpoint = torch.load(\"./BERTModified-fullsize-kg-finetuned-wikitext-test/\"+most_updated_checkp+\"/pytorch_model.bin\")\n",
    "BERTModified_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2e30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollator(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "# DataCollatorForWholeWordMask(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "403d183b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To test model with smaller sample dataset\n",
    "\n",
    "train_size = 100\n",
    "test_size = 50\n",
    "\n",
    "downsampled_dataset = final_dataset[\"train\"].train_test_split(train_size=train_size, test_size=test_size, seed=42)\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25c77502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# metrics = evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds=None, logits=None, labels=None):\n",
    "    \n",
    "    # We should have either `eval_preds` or both `logits` and `labels`\n",
    "    if eval_preds:\n",
    "        logits, labels = eval_preds\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    ## Flatten values\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a8d2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset['train']) // batch_size #len(final_dataset['train']) // batch_size\n",
    "model_name = \"BERTModified-rawbert\"\n",
    "output_dir = f\"{model_name}-finetuned-wikitext-test\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "#     fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=20,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"loss\",#metric_name,\n",
    "#     greater_is_better = False,\n",
    "    logging_dir='logs',\n",
    "    report_to=\"wandb\",\n",
    "#     no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ed9ae",
   "metadata": {},
   "source": [
    "metric_for_best_model (str, optional) — Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\". Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "\n",
    "If you set this value, greater_is_better will default to True. Don’t forget to set it to False if your metric is better when lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd221993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/KG-LM-Integration/BERTModified-rawbert-finetuned-wikitext-test is already a clone of https://huggingface.co/HideOnBush/BERTModified-rawbert-finetuned-wikitext-test. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=BERTModified_model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"], #final_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"], #final_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5f89c",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a3305e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33medwardjian\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/xiangru/KG-LM-Integration/wandb/run-20221209_111853-t1f5guq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/edwardjian/huggingface/runs/t1f5guq4\" target=\"_blank\">BERTModified-rawbert-finetuned-wikitext-test</a></strong> to <a href=\"https://wandb.ai/edwardjian/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 2.56\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b7f5033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9397574067115784,\n",
       " 'eval_precision': 0.8162031438935913,\n",
       " 'eval_recall': 0.8162031438935913,\n",
       " 'eval_f1': 0.8162031438935913,\n",
       " 'eval_accuracy': 0.8162031438935913,\n",
       " 'eval_runtime': 2.8507,\n",
       " 'eval_samples_per_second': 17.54,\n",
       " 'eval_steps_per_second': 4.56}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49714c",
   "metadata": {},
   "source": [
    "# TODO: Trainer.model is not a `PreTrainedModel`, only saving its state dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12cfd7d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12500\n",
      "  Number of trainable parameters = 71331479\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 49:37, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.190600</td>\n",
       "      <td>10.064298</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.675700</td>\n",
       "      <td>9.844508</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.261700</td>\n",
       "      <td>9.678900</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.932400</td>\n",
       "      <td>9.583185</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.679200</td>\n",
       "      <td>9.510508</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.467300</td>\n",
       "      <td>9.445129</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.293400</td>\n",
       "      <td>9.430649</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.151100</td>\n",
       "      <td>9.421928</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.024300</td>\n",
       "      <td>9.385569</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.916900</td>\n",
       "      <td>9.412608</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.815000</td>\n",
       "      <td>9.374329</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.726400</td>\n",
       "      <td>9.359411</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.646400</td>\n",
       "      <td>9.390010</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.566000</td>\n",
       "      <td>9.458851</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.499800</td>\n",
       "      <td>9.384863</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.429700</td>\n",
       "      <td>9.411346</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.380600</td>\n",
       "      <td>9.434789</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.322500</td>\n",
       "      <td>9.467047</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.266900</td>\n",
       "      <td>9.521500</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.212300</td>\n",
       "      <td>9.518990</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>7.166500</td>\n",
       "      <td>9.560420</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7.128200</td>\n",
       "      <td>9.615518</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>7.081300</td>\n",
       "      <td>9.573235</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.052800</td>\n",
       "      <td>9.638080</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.013800</td>\n",
       "      <td>9.578024</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.980600</td>\n",
       "      <td>9.603198</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6.961700</td>\n",
       "      <td>9.631948</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>6.923200</td>\n",
       "      <td>9.607459</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>6.893600</td>\n",
       "      <td>9.667953</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.875900</td>\n",
       "      <td>9.712850</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>6.837900</td>\n",
       "      <td>9.767794</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>6.806800</td>\n",
       "      <td>9.716960</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>6.790300</td>\n",
       "      <td>9.701800</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.781200</td>\n",
       "      <td>9.814104</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>6.754900</td>\n",
       "      <td>9.770327</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>6.733700</td>\n",
       "      <td>9.747643</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.723900</td>\n",
       "      <td>9.765244</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>6.715000</td>\n",
       "      <td>9.831970</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>6.689700</td>\n",
       "      <td>9.888529</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.691000</td>\n",
       "      <td>9.895905</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>6.658400</td>\n",
       "      <td>9.851236</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>6.658700</td>\n",
       "      <td>9.862288</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>6.654300</td>\n",
       "      <td>9.923895</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>6.650600</td>\n",
       "      <td>9.867826</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>6.631300</td>\n",
       "      <td>9.882892</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>6.631900</td>\n",
       "      <td>10.006773</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>6.620800</td>\n",
       "      <td>9.955661</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>6.613500</td>\n",
       "      <td>9.863731</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>6.620800</td>\n",
       "      <td>9.910364</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.622700</td>\n",
       "      <td>9.904378</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-12000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-12250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-12500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=7.328463642578125, metrics={'train_runtime': 2977.3358, 'train_samples_per_second': 16.794, 'train_steps_per_second': 4.198, 'total_flos': 0.0, 'train_loss': 7.328463642578125, 'epoch': 50.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "#trainer.save_model(\"output/models/BERTModified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b4ed6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 161407174.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.89944076538086,\n",
       " 'eval_precision': 0.25,\n",
       " 'eval_recall': 0.25,\n",
       " 'eval_f1': 0.25,\n",
       " 'eval_accuracy': 0.25,\n",
       " 'eval_runtime': 0.5253,\n",
       " 'eval_samples_per_second': 19.036,\n",
       " 'eval_steps_per_second': 5.711,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") #21596 for 1 epoch\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec0488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Need to login to huggingface to push to hub\n",
    "\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd9be2",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebb2e81f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 128, 30522) (50, 128)\n"
     ]
    }
   ],
   "source": [
    "## errors in GPU (then pc cant find GPU anymore)\n",
    "predictions = trainer.predict(downsampled_dataset[\"test\"]) #final_dataset[\"valid\"] cpu crashes\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf058556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8754,  2009,  2160, ...,  2394,  8676,  2015],\n",
       "       [ 1999,  2035,  2660, ...,  2160,  2660,  1998],\n",
       "       [ 1999,  2240,  1999, ...,  2075, 12256,  8540],\n",
       "       ...,\n",
       "       [ 7044,  4605,  1010, ...,  2010,  2417,  9899],\n",
       "       [ 2148,  4895,  2380, ...,  1996,  8842,  1997],\n",
       "       [ 2039,  1037,  5154, ...,  2473,  2027,  2370]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfc00b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  8540, 12256, 26775],\n",
       "       ...,\n",
       "       [ 7044,  4605,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  8842,  1997],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f585e",
   "metadata": {},
   "source": [
    "[About micro, macro, weighted precision, recall, f1 for multiclass labels](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)\n",
    "\n",
    "The following always holds true for the micro-F1 case:\n",
    "\n",
    "`micro-F1 = micro-precision = micro-recall = accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de842903",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.8174123337363967,\n",
       " 'recall': 0.8174123337363967,\n",
       " 'f1': 0.8174123337363967,\n",
       " 'accuracy': 0.8174123337363967}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits = predictions.predictions, labels = predictions.label_ids)\n",
    "# metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66277710",
   "metadata": {},
   "source": [
    "## Test KG entities prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1969bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTModified(nn.Module): #PreTrainedModel\n",
    "    def __init__(self, bert_model_name, base_model, config):\n",
    "        \n",
    "#         super().__init__(config) #For PreTrainedModel\n",
    "        super().__init__() ## for nn.Module\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.config = config\n",
    "        self.kg_size = qids_wktxt.shape[0]\n",
    "                \n",
    "        self.activation = GELUActivation() # for distilbert\n",
    "        self.vocab_transform = nn.Linear(self.config.dim, self.config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(self.config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "        \n",
    "        self.kg_projector = nn.Linear(self.config.dim, self.kg_size)\n",
    "        \n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ## set to eval\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        ## freeze model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        ## initialization of integrator\n",
    "        self.itgt = WordItgtor(self.config.dim,200)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kg_embedding = None,           ## given\n",
    "        kg_embedding_mask = None,      ## given\n",
    "        kg_embedding_mask_qid = None,      ## given\n",
    "        kg_embedding_mask_index = None,\n",
    "        input_ids = None,              ## given\n",
    "        attention_mask = None,         ## given\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,                 ## given\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,):\n",
    "        \n",
    "        base_model_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        ## Get LM embedding\n",
    "        hidden_states = base_model_output[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "\n",
    "        \n",
    "        ## TODO: Use hidden_states and kg_embedding and perform INTEGRATION\n",
    "#         prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        \n",
    "        \n",
    "        kg_embedding = torch.tensor(kg_embedding).to(device='cuda:0')\n",
    "        kg_embedding_mask_index = torch.tensor(kg_embedding_mask_index).to(device='cuda:0')\n",
    "        \n",
    "        prediction_logits = self.itgt(hidden_states, kg_embedding)\n",
    "        prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        lm_prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        kg_prediction_logits = self.kg_projector(prediction_logits)  # (bs, seq_length, kg_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        kg_loss = None\n",
    "        \n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(lm_prediction_logits.view(-1, lm_prediction_logits.size(-1)), labels.view(-1))\n",
    "        if kg_embedding_mask_index is not None:\n",
    "            kg_loss = self.mlm_loss_fct(kg_prediction_logits.view(-1, kg_prediction_logits.size(-1)), kg_embedding_mask_index.view(-1))\n",
    "\n",
    "        total_loss = mlm_loss + kg_loss\n",
    "        \n",
    "        return MaskedLMOutput(\n",
    "            loss=total_loss,\n",
    "            logits=kg_prediction_logits,\n",
    "            hidden_states=base_model_output.hidden_states,\n",
    "            attentions=base_model_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54a5d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xiangru/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xiangru/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "BERTModified_model = BERTModified(bert_model_name = bert_model_name,\n",
    "                                  base_model = base_model,\n",
    "                                  config = base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "559a9831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_updated_checkp = \"checkpoint-65730\"\n",
    "checkpoint = torch.load(\"./BERTModified-fullsize-kg-finetuned-wikitext-test/\"+most_updated_checkp+\"/pytorch_model.bin\")\n",
    "BERTModified_model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fab33ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "#     fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=50,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"loss\",#metric_name,\n",
    "#     greater_is_better = False,\n",
    "    logging_dir='logs',\n",
    "    report_to=\"wandb\",\n",
    "#     no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11b16c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/KG-LM-Integration/BERTModified-rawbert-finetuned-wikitext-test is already a clone of https://huggingface.co/HideOnBush/BERTModified-rawbert-finetuned-wikitext-test. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=BERTModified_model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"], #final_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"], #final_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "324e450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 50\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(downsampled_dataset[\"test\"]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cab3f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[46409, 46125, 44768, ..., 45279, 45279, 44768],\n",
       "       [46508, 32203, 36858, ..., 45279, 43331, 44136],\n",
       "       [40159, 35582, 40277, ..., 17980, 17980, 17980],\n",
       "       ...,\n",
       "       [46017, 46017, 46464, ..., 39120, 46546, 46546],\n",
       "       [40222, 42290, 35582, ..., 44496,  8797,  8797],\n",
       "       [46653, 43418, 46653, ..., 41482, 44046, 27681]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted label of entitles\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d5296da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ..., 17980, 17980, 17980],\n",
       "       ...,\n",
       "       [46017, 46017,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  8797,  8797],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# true label of entitles\n",
    "labels = np.array(downsampled_dataset[\"test\"][\"kg_embedding_mask_index\"])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ff128d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'accuracy': 1.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits = predictions.predictions, labels =labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f15f3",
   "metadata": {},
   "source": [
    "## Try from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0daaffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # Initialize MLM pipeline\n",
    "# mlm = pipeline('fill-mask', model=BERTModified_model, tokenizer=my_tokenizer)\n",
    "\n",
    "# # Get mask token\n",
    "# mask = mlm.tokenizer.mask_token\n",
    "\n",
    "# # Get result for particular masked phrase\n",
    "# phrase = f'Wikipedia is a free online {mask}, created and edited by volunteers around the world'\n",
    "\n",
    "# result = mlm(phrase)\n",
    "\n",
    "# # Print result\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in result:\n",
    "#     print(f\">>> {x['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c32330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6883f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
