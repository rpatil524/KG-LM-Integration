{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f5d47c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415ab001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/xiangru/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import gc\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
    "from transformers.activations import GELUActivation\n",
    "from transformers.modeling_outputs import MaskedLMOutput\n",
    "from transformers import DataCollatorForWholeWordMask\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers.data.data_collator import _torch_collate_batch\n",
    "import evaluate\n",
    "\n",
    "import wandb\n",
    "# wandb.init(project=\"kg-lm-integration\", entity=\"tanny411\")\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#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\n",
    "emb_tsv_file = \"wikidata_translation_v1.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d2eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.getrecursionlimit())\n",
    "\n",
    "sys.setrecursionlimit(100000)\n",
    "# but doing so is dangerous -- the standard limit is a little conservative, but Python stackframes can be quite big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed5a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.current_device(), torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a1447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"distilbert-base-uncased\" ##\"bert-base-cased\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6a6ac",
   "metadata": {},
   "source": [
    "# Initialize Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d8ed2b",
   "metadata": {},
   "source": [
    "Download:\n",
    "- embeds_wktxt.csv\n",
    "- [linked-wikitext-2 dataset](https://rloganiv.github.io/linked-wikitext-2/#/) and unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b41ccbd",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8495d87e",
   "metadata": {},
   "source": [
    "- `tokens` are the given list of tokens from wikitext2\n",
    "- `input_ids` are what come from tokenization, they divide certain words into multiple pieces, and each sentence has a CLS and a SEP\n",
    "- `word_tokens` is the length of `tokens`. For each token in `token`, it mentions how many sub-words it was divided into due to word piece tokenization\n",
    "- `cummulative_word_tokens` is a cummulative sum of `word_tokens`, with an extra 0 in the beginning\n",
    "\n",
    "##### Process\n",
    "index of a token in `token` can be found in `input_ids` by `cummulative_word_tokens`. if `ix` is the index of a word in `token`, its beginning index in `input_ids` is `cummulative_word_tokens[ix] + 1`, the +1 is because `input_ids` has a CLS in the beginning. `token[ix]` spans from `input_ids[cummulative_word_tokens[ix] + 1]` to `input_ids[cummulative_word_tokens[ix+1] + 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efb55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0facda0eb7ba06ee\n",
      "Found cached dataset json (/home/xiangru/.cache/huggingface/datasets/json/default-0facda0eb7ba06ee/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124fdc71ce334927987cc21a2fd736e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeds_wktxt = pd.read_csv(\"embeds_wktxt.csv\")\n",
    "qids_wktxt = pd.read_csv(\"qids_wktxt2.csv\")\n",
    "\n",
    "linked_wikitext_2 = \"linked-wikitext-2/\"\n",
    "train = linked_wikitext_2+\"train.jsonl\"\n",
    "valid = linked_wikitext_2+\"valid.jsonl\"\n",
    "test = linked_wikitext_2+\"test.jsonl\"\n",
    "\n",
    "data_files = {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "wikitest2_dataset = load_dataset(\"json\", data_files=data_files)\n",
    "\n",
    "chunk_size = 128\n",
    "\n",
    "class BertTokenizerModified(DistilBertTokenizer): #BertTokenizer\n",
    "    def __init__(self,vocab_file,**kwargs):\n",
    "        \n",
    "        super().__init__(vocab_file, never_split=[\"@@START@@\", \"@@END@@\", \"@@start@@\", \"@@end@@\"], **kwargs)\n",
    "    \n",
    "        self.tokenized_list = []\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        token_list = text.split()\n",
    "        split_tokens = []\n",
    "        tokenized_list = []\n",
    "        \n",
    "        if self.do_basic_tokenize:\n",
    "            for token in token_list:\n",
    "\n",
    "                # If the token is part of the never_split set\n",
    "                if token in self.basic_tokenizer.never_split:\n",
    "                    split_tokens.append(token)\n",
    "                    tokenized_list.append(1)\n",
    "                else:\n",
    "                    word_tokenized = self.wordpiece_tokenizer.tokenize(token)\n",
    "                    split_tokens += word_tokenized\n",
    "                    tokenized_list.append(len(word_tokenized))\n",
    "\n",
    "        self.tokenized_list.append(tokenized_list)\n",
    "        return split_tokens\n",
    "    \n",
    "def get_cumm(vals):\n",
    "    cumm = 0\n",
    "    res = [0] ## len of res is 1 more than vals, with an initial 0\n",
    "    for val in vals:\n",
    "        cumm += val\n",
    "        res.append(cumm)\n",
    "    return res\n",
    "\n",
    "def my_tokenize_function(data):\n",
    "    \n",
    "    ## tokenize\n",
    "    my_tokenizer.tokenized_list = []\n",
    "    result = my_tokenizer([\" \".join(eg) for eg in data[\"tokens\"]])\n",
    "    if my_tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    \n",
    "    ## save word to token mapping\n",
    "    ## 3, 1, 1 means the first word got divided into 3 tokens, the next into 1, and the next into 1 again\n",
    "    result[\"word_tokens\"] = my_tokenizer.tokenized_list\n",
    "    result[\"cummulative_word_tokens\"] = [get_cumm(x) for x in result[\"word_tokens\"]]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_kg_embedding_batched(data):\n",
    "    \n",
    "    ## store a masking array that says whether or not an item has kg embedding\n",
    "    \"\"\"\n",
    "    When you specify batched=True the function receives a dictionary with the fields of the dataset, \n",
    "    but each value is now a list of values, and not just a single value. \n",
    "    \"\"\"\n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    annotations_list = data['annotations']\n",
    "    cummulative_word_tokens_list = data[\"cummulative_word_tokens\"]\n",
    "    \n",
    "    batch_size = len(input_ids_list)\n",
    "    embed_list = [] ## len will be batch_size\n",
    "    embed_mask = []\n",
    "    embed_mask_qid = []\n",
    "    \n",
    "    #add by Edward, the index of qid\n",
    "    embed_mask_index = []\n",
    "    \n",
    "    allc = 0\n",
    "    cc = 0\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        input_ids = input_ids_list[i]\n",
    "        annotations = annotations_list[i]\n",
    "        \n",
    "        ## Replace zeros with random numbers if required\n",
    "        embeds = np.zeros((len(input_ids), 200)) ## CLS, SEP will have np.zeros, like unknown words\n",
    "        mask = [0]*len(input_ids)\n",
    "        mask_qid = ['0']*len(input_ids)\n",
    "        \n",
    "        #add by Edward\n",
    "        mask_index = [-100]*len(input_ids)\n",
    "        \n",
    "        \n",
    "        for annot in annotations:\n",
    "            start_ix, end_ix = annot['span']\n",
    "            start = cummulative_word_tokens_list[i][start_ix] + 1\n",
    "            end = cummulative_word_tokens_list[i][end_ix] + 1\n",
    "            \n",
    "            qid = annot['id']\n",
    "            \n",
    "            #add by Edward\n",
    "            index_list = qids_wktxt[qids_wktxt[\"id\"]==qid].index.tolist()\n",
    "            allc += 1\n",
    "            if len(index_list) == 0:\n",
    "                qid_index = -100\n",
    "\n",
    "            else:\n",
    "                qid_index = index_list[0]\n",
    "                cc+=1\n",
    "            \n",
    "            df = embeds_wktxt[embeds_wktxt['id']==qid]\n",
    "            if len(df)>0:\n",
    "                embeds[start:end] = np.tile(df.iloc[0,1:].values.reshape((1,200)),(end-start, 1))\n",
    "                mask[start:end] = [1]*(end-start)\n",
    "                mask_qid[start:end] = [qid]*(end-start)\n",
    "                \n",
    "                #add by Edward\n",
    "                mask_index[start:end] = [qid_index]*(end-start)\n",
    "                \n",
    "                \n",
    "        embed_mask.append(mask)\n",
    "        embed_mask_qid.append(mask_qid)\n",
    "        embed_list.append(embeds)\n",
    "        \n",
    "        #add by Edward\n",
    "        embed_mask_index.append(mask_index)\n",
    "\n",
    "    \n",
    "    print(cc/allc)\n",
    "    return {\n",
    "        \"kg_embedding\": embed_list, \n",
    "        \"kg_embedding_mask\": embed_mask,\n",
    "        \"kg_embedding_mask_qid\": embed_mask_qid,\n",
    "        \"kg_embedding_mask_index\": embed_mask_index\n",
    "    }\n",
    "\n",
    "def filter_text_batched(data):\n",
    "    \n",
    "    new_data = {k:[] for k in data}\n",
    "    \n",
    "    input_ids_list = data[\"input_ids\"]\n",
    "    \n",
    "    ## remove [UNK] == 100 \n",
    "    indices_list = [[i for i,input_id in enumerate(input_ids) if input_id!=100]\n",
    "                        for input_ids in input_ids_list]\n",
    "    \n",
    "    for k in data:\n",
    "        for indices, data_list in zip(indices_list, data[k]):\n",
    "            new_data[k].append([data_list[ind] for ind in indices])\n",
    "        \n",
    "    return new_data\n",
    "\n",
    "def truncate_data(data):\n",
    "    maxlength = my_tokenizer.max_model_input_sizes[bert_model_name]\n",
    "\n",
    "    ## truncate to maxlength\n",
    "    for k in data:\n",
    "        data[k] = [x[:maxlength] for x in data[k]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # Create a new labels column\n",
    "#     result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6ae3812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerModified'.\n"
     ]
    }
   ],
   "source": [
    "my_tokenizer = BertTokenizerModified.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2f2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = \"concat_dataset_v2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538eaf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_dataset = wikitest2_dataset.map(my_tokenize_function, batched=True)\\\n",
    "                          .map(get_kg_embedding_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .remove_columns(['title', 'tokens', 'annotations', 'word_tokens', 'cummulative_word_tokens'])\\\n",
    "                          .map(filter_text_batched, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "                          .map(group_texts, batched=True, batch_size=100, keep_in_memory=False)\\\n",
    "\n",
    "final_dataset.save_to_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae5e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the saved tokenized dataset\n",
    "final_dataset = load_from_disk(dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b036f9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 17528\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 1798\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 2069\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2314fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Data Collator for Masking\n",
    "\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from collections.abc import Mapping\n",
    "\n",
    "class CustomDataCollator(DataCollatorForWholeWordMask):\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        if isinstance(examples[0], Mapping):\n",
    "            input_ids = [e[\"input_ids\"] for e in examples]\n",
    "            kg_embedding_mask = [e[\"kg_embedding_mask\"] for e in examples]\n",
    "            kg_embedding = [e[\"kg_embedding\"] for e in examples]\n",
    "            kg_embedding_mask_qid = [e[\"kg_embedding_mask_qid\"] for e in examples]\n",
    "            \n",
    "            #add by Edward\n",
    "            kg_embedding_mask_index = [e[\"kg_embedding_mask_index\"] for e in examples]\n",
    "        else:\n",
    "            raise Exception(\"Dataset needs to be in dictionary format\")\n",
    "\n",
    "        batch_input = _torch_collate_batch(input_ids, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        mask_labels = kg_embedding_mask\n",
    "        batch_mask = _torch_collate_batch(mask_labels, self.tokenizer, pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "        inputs, labels = self.torch_mask_tokens(batch_input, batch_mask)\n",
    "        \n",
    "        return {\n",
    "                \"input_ids\": inputs, \n",
    "                \"labels\": labels, \n",
    "                \"kg_embedding\":kg_embedding, \n",
    "                \"kg_embedding_mask\":kg_embedding_mask,\n",
    "                \"kg_embedding_mask_qid\":kg_embedding_mask_qid,\n",
    "                \"kg_embedding_mask_index\":kg_embedding_mask_index,\n",
    "               }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14e07f",
   "metadata": {},
   "source": [
    "# Integrator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfc69fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class WordItgtor(nn.Module):\n",
    "    \n",
    "    # seems like the dimension of the language model embedding will be 768 while that for \n",
    "    # knowledge graph embedding is 200, the module is kind of built based on this fact that \n",
    "    # the former has larger dimension\n",
    "    def __init__(self, embed_dim_lm, embed_dim_kg):\n",
    "        super(WordItgtor, self).__init__()\n",
    "        \n",
    "        self.tt_embed_dim = embed_dim_lm, embed_dim_kg\n",
    "        \n",
    "        self.fc_kg = nn.Linear(embed_dim_kg,embed_dim_lm)\n",
    "        self.fc_lm = nn.Linear(embed_dim_lm,embed_dim_lm)\n",
    "        self.fc1 = nn.Linear(embed_dim_lm * 2, embed_dim_lm * 4)\n",
    "        self.fc2 = nn.Linear(embed_dim_lm * 4, embed_dim_lm * 2)\n",
    "        self.fc3 = nn.Linear(embed_dim_lm * 2, embed_dim_lm)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x_lm, x_kg, kg_mask=None):        \n",
    "        # x_lm_raw is the embedding of a single sentence from language model with size \n",
    "        # ((number of words) * embed_dim_lm)\n",
    "        \n",
    "        # x_kg is the embedding of a single sentence from knowledge graph with size \n",
    "        # ((number of words) * embed_dim_lm)\n",
    "        \n",
    "#         if kg_mask == 0:\n",
    "#             return x_lm\n",
    "        \n",
    "        x_kg = self.fc_kg(x_kg)\n",
    "        x_lm = self.fc_lm(x_lm)\n",
    "        \n",
    "        x = torch.cat((x_lm,x_kg),dim=-1)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166a7e57",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "682f14fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedModel\n",
    "\n",
    "class BERTModified(nn.Module): #PreTrainedModel\n",
    "    def __init__(self, bert_model_name, base_model, config):\n",
    "        \n",
    "#         super().__init__(config) #For PreTrainedModel\n",
    "        super().__init__() ## for nn.Module\n",
    "        \n",
    "        self.base_model = base_model\n",
    "        self.config = config\n",
    "        self.kg_size = qids_wktxt.shape[0]\n",
    "                \n",
    "        self.activation = GELUActivation() # for distilbert\n",
    "        self.vocab_transform = nn.Linear(self.config.dim, self.config.dim)\n",
    "        self.vocab_layer_norm = nn.LayerNorm(self.config.dim, eps=1e-12)\n",
    "        self.vocab_projector = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "        \n",
    "        self.kg_projector = nn.Linear(self.config.dim, self.kg_size)\n",
    "        \n",
    "        self.mlm_loss_fct = nn.CrossEntropyLoss()\n",
    "        \n",
    "        ## set to eval\n",
    "        self.base_model.eval()\n",
    "        \n",
    "        ## freeze model\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        ## initialization of integrator\n",
    "        self.itgt = WordItgtor(self.config.dim,200)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        kg_embedding = None,           ## given\n",
    "        kg_embedding_mask = None,      ## given\n",
    "        kg_embedding_mask_qid = None,      ## given\n",
    "        kg_embedding_mask_index = None,\n",
    "        input_ids = None,              ## given\n",
    "        attention_mask = None,         ## given\n",
    "        head_mask = None,\n",
    "        inputs_embeds = None,\n",
    "        labels = None,                 ## given\n",
    "        output_attentions = None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,):\n",
    "        \n",
    "        base_model_output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        ## Get LM embedding\n",
    "        hidden_states = base_model_output[0]  # (bs, seq_length, dim)\n",
    "        \n",
    "\n",
    "        \n",
    "        ## TODO: Use hidden_states and kg_embedding and perform INTEGRATION\n",
    "#         prediction_logits = self.vocab_transform(hidden_states)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "#         prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        \n",
    "        \n",
    "        kg_embedding = torch.tensor(kg_embedding).to(device='cuda:0')\n",
    "        kg_embedding_mask_index = torch.tensor(kg_embedding_mask_index).to(device='cuda:0')\n",
    "        \n",
    "        prediction_logits = self.itgt(hidden_states, kg_embedding)\n",
    "        prediction_logits = self.activation(prediction_logits)  # (bs, seq_length, dim)\n",
    "        prediction_logits = self.vocab_layer_norm(prediction_logits)  # (bs, seq_length, dim)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "        lm_prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n",
    "        kg_prediction_logits = self.kg_projector(prediction_logits)  # (bs, seq_length, kg_size)\n",
    "\n",
    "        mlm_loss = None\n",
    "        kg_loss = None\n",
    "        \n",
    "\n",
    "        \n",
    "        if labels is not None:\n",
    "            mlm_loss = self.mlm_loss_fct(lm_prediction_logits.view(-1, lm_prediction_logits.size(-1)), labels.view(-1))\n",
    "        if kg_embedding_mask_index is not None:\n",
    "            kg_loss = self.mlm_loss_fct(kg_prediction_logits.view(-1, kg_prediction_logits.size(-1)), kg_embedding_mask_index.view(-1))\n",
    "\n",
    "        total_loss = mlm_loss + kg_loss\n",
    "        \n",
    "        return MaskedLMOutput(\n",
    "            loss=total_loss,\n",
    "            logits=lm_prediction_logits,\n",
    "            hidden_states=base_model_output.hidden_states,\n",
    "            attentions=base_model_output.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4ac2e8e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/xiangru/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/config.json\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/xiangru/.cache/huggingface/hub/models--distilbert-base-uncased/snapshots/1c4513b2eedbda136f57676a34eea67aba266e5c/pytorch_model.bin\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModel.from_pretrained(bert_model_name)\n",
    "BERTModified_model = BERTModified(bert_model_name = bert_model_name,\n",
    "                                  base_model = base_model,\n",
    "                                  config = base_model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0338cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERTModified_model = AutoModel.from_pretrained(\"Aisha/BERTModified-finetuned-wikitext-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2e30fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = CustomDataCollator(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)\n",
    "# DataCollatorForWholeWordMask(tokenizer=my_tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "403d183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at concat_dataset_v4/train/cache-f6c579ef53c2b0e6.arrow and concat_dataset_v4/train/cache-f218f6d62d6d3c47.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'kg_embedding', 'kg_embedding_mask', 'kg_embedding_mask_qid', 'kg_embedding_mask_index'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To test model with smaller sample dataset\n",
    "\n",
    "train_size = 1000\n",
    "test_size = 10\n",
    "\n",
    "downsampled_dataset = final_dataset[\"train\"].train_test_split(train_size=train_size, test_size=test_size, seed=42)\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25c77502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# metrics = evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"f1\"])\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_preds=None, logits=None, labels=None):\n",
    "    \n",
    "    # We should have either `eval_preds` or both `logits` and `labels`\n",
    "    if eval_preds:\n",
    "        logits, labels = eval_preds\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[l for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    ## Flatten values\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    accuracy = accuracy_metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    precision = precision_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    recall = recall_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    f1 = f1_metric.compute(predictions=true_predictions, references=true_labels, average=\"micro\")\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision[\"precision\"],\n",
    "        \"recall\": recall[\"recall\"],\n",
    "        \"f1\": f1[\"f1\"],\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5a8d2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Show the training loss with every epoch\n",
    "logging_steps = len(downsampled_dataset['train']) // batch_size #len(final_dataset['train']) // batch_size\n",
    "model_name = \"BERTModified-rawbert\"\n",
    "output_dir = f\"{model_name}-finetuned-wikitext-test\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,\n",
    "#     fp16=True,\n",
    "    logging_steps=logging_steps,\n",
    "    num_train_epochs=50,\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"loss\",#metric_name,\n",
    "#     greater_is_better = False,\n",
    "    logging_dir='logs',\n",
    "    report_to=\"wandb\",\n",
    "#     no_cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ed9ae",
   "metadata": {},
   "source": [
    "metric_for_best_model (str, optional) — Use in conjunction with load_best_model_at_end to specify the metric to use to compare two different models. Must be the name of a metric returned by the evaluation with or without the prefix \"eval_\". Will default to \"loss\" if unspecified and load_best_model_at_end=True (to use the evaluation loss).\n",
    "\n",
    "If you set this value, greater_is_better will default to True. Don’t forget to set it to False if your metric is better when lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd221993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/huggingface_hub/repository.py:708: FutureWarning: Creating a repository through 'clone_from' is deprecated and will be removed in v0.11.\n",
      "  FutureWarning,\n",
      "Cloning https://huggingface.co/HideOnBush/BERTModified-rawbert-finetuned-wikitext-test into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=BERTModified_model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"], #final_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"], #final_dataset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b027762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = [trainer.eval_dataset[i] for i in range(2)]\n",
    "# dc = data_collator(samples)\n",
    "# kg_embedding_mask = [sample['kg_embedding_mask'] for sample in samples]\n",
    "\n",
    "# for id_list, label_list, mask_list in zip(dc[\"input_ids\"], dc[\"labels\"], kg_embedding_mask):\n",
    "#     tokens = my_tokenizer.convert_ids_to_tokens(id_list)\n",
    "#     labels = my_tokenizer.convert_ids_to_tokens(label_list)\n",
    "#     for token, label, mask in zip(tokens, labels, mask_list):\n",
    "#         print(token, '\\t', label, '\\t', mask)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a5f89c",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7a3305e2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:47]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 30289.83\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4b7f5033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 10.318567276000977,\n",
       " 'eval_precision': 0.0,\n",
       " 'eval_recall': 0.0,\n",
       " 'eval_f1': 0.0,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 0.5117,\n",
       " 'eval_samples_per_second': 19.541,\n",
       " 'eval_steps_per_second': 5.862}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49714c",
   "metadata": {},
   "source": [
    "# TODO: Trainer.model is not a `PreTrainedModel`, only saving its state dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "12cfd7d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangru/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12500\n",
      "  Number of trainable parameters = 71331479\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 49:37, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10.190600</td>\n",
       "      <td>10.064298</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>9.675700</td>\n",
       "      <td>9.844508</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9.261700</td>\n",
       "      <td>9.678900</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>8.932400</td>\n",
       "      <td>9.583185</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>8.679200</td>\n",
       "      <td>9.510508</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.467300</td>\n",
       "      <td>9.445129</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8.293400</td>\n",
       "      <td>9.430649</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8.151100</td>\n",
       "      <td>9.421928</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>8.024300</td>\n",
       "      <td>9.385569</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.916900</td>\n",
       "      <td>9.412608</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>7.815000</td>\n",
       "      <td>9.374329</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>7.726400</td>\n",
       "      <td>9.359411</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>7.646400</td>\n",
       "      <td>9.390010</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>7.566000</td>\n",
       "      <td>9.458851</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>7.499800</td>\n",
       "      <td>9.384863</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>7.429700</td>\n",
       "      <td>9.411346</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>7.380600</td>\n",
       "      <td>9.434789</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>7.322500</td>\n",
       "      <td>9.467047</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>7.266900</td>\n",
       "      <td>9.521500</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.212300</td>\n",
       "      <td>9.518990</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>7.166500</td>\n",
       "      <td>9.560420</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>7.128200</td>\n",
       "      <td>9.615518</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>7.081300</td>\n",
       "      <td>9.573235</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>7.052800</td>\n",
       "      <td>9.638080</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>7.013800</td>\n",
       "      <td>9.578024</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.980600</td>\n",
       "      <td>9.603198</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>6.961700</td>\n",
       "      <td>9.631948</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>6.923200</td>\n",
       "      <td>9.607459</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "      <td>0.032051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>6.893600</td>\n",
       "      <td>9.667953</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.875900</td>\n",
       "      <td>9.712850</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>6.837900</td>\n",
       "      <td>9.767794</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>6.806800</td>\n",
       "      <td>9.716960</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>6.790300</td>\n",
       "      <td>9.701800</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>6.781200</td>\n",
       "      <td>9.814104</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>6.754900</td>\n",
       "      <td>9.770327</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>6.733700</td>\n",
       "      <td>9.747643</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.723900</td>\n",
       "      <td>9.765244</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>6.715000</td>\n",
       "      <td>9.831970</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>6.689700</td>\n",
       "      <td>9.888529</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6.691000</td>\n",
       "      <td>9.895905</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>6.658400</td>\n",
       "      <td>9.851236</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>6.658700</td>\n",
       "      <td>9.862288</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>6.654300</td>\n",
       "      <td>9.923895</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>6.650600</td>\n",
       "      <td>9.867826</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>6.631300</td>\n",
       "      <td>9.882892</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>6.631900</td>\n",
       "      <td>10.006773</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>6.620800</td>\n",
       "      <td>9.955661</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.025641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>6.613500</td>\n",
       "      <td>9.863731</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>6.620800</td>\n",
       "      <td>9.910364</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.622700</td>\n",
       "      <td>9.904378</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.051282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-1750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-2750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-3750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-4750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-5750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-6750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-7750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-8750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-9750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-10750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-11750\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-12000\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-12250\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to BERTModified-rawbert-finetuned-wikitext-test/checkpoint-12500\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12500, training_loss=7.328463642578125, metrics={'train_runtime': 2977.3358, 'train_samples_per_second': 16.794, 'train_steps_per_second': 4.198, 'total_flos': 0.0, 'train_loss': 7.328463642578125, 'epoch': 50.0})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "#trainer.save_model(\"output/models/BERTModified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b4ed6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 161407174.64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 18.89944076538086,\n",
       " 'eval_precision': 0.25,\n",
       " 'eval_recall': 0.25,\n",
       " 'eval_f1': 0.25,\n",
       " 'eval_accuracy': 0.25,\n",
       " 'eval_runtime': 0.5253,\n",
       " 'eval_samples_per_second': 19.036,\n",
       " 'eval_steps_per_second': 5.711,\n",
       " 'epoch': 50.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\") #21596 for 1 epoch\n",
    "\n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bec0488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Need to login to huggingface to push to hub\n",
    "\n",
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd9be2",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ebb2e81f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 10\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 128, 30522) (10, 128)\n"
     ]
    }
   ],
   "source": [
    "## errors in GPU (then pc cant find GPU anymore)\n",
    "predictions = trainer.predict(downsampled_dataset[\"test\"]) #final_dataset[\"valid\"] cpu crashes\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf058556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1011,  2009,  1005, ...,  2139,  2006,  2015],\n",
       "       [ 2015,  1998,  2015, ...,  1997,  2455,  1998],\n",
       "       [ 1998,  2009, 29625, ...,  1005,  1038,  2243],\n",
       "       ...,\n",
       "       [ 2189,  1997,  2307, ...,  1997,  2032,  1997],\n",
       "       [ 2399,  2009,  1998, ...,  2032,  1999,  2032],\n",
       "       [ 2051,  1996,  2274, ...,  1996,  2923,  1998]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfc00b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  8540, 12256, 26775],\n",
       "       ...,\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100],\n",
       "       [15756,  6125,  -100, ...,  -100,  -100,  -100],\n",
       "       [ -100,  -100,  -100, ...,  -100,  -100,  -100]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f585e",
   "metadata": {},
   "source": [
    "[About micro, macro, weighted precision, recall, f1 for multiclass labels](https://towardsdatascience.com/multi-class-metrics-made-simple-part-ii-the-f1-score-ebe8b2c2ca1)\n",
    "\n",
    "The following always holds true for the micro-F1 case:\n",
    "\n",
    "`micro-F1 = micro-precision = micro-recall = accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de842903",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.22435897435897437,\n",
       " 'recall': 0.22435897435897437,\n",
       " 'f1': 0.22435897435897437,\n",
       " 'accuracy': 0.22435897435897437}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(logits = predictions.predictions, labels = predictions.label_ids)\n",
    "# metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f15f3",
   "metadata": {},
   "source": [
    "## Try from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0daaffec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11977/1041909434.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Wikipedia is a free online {mask}, created and edited by volunteers around the world'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Print result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mto\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmasked\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/pipelines/fill_mask.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11977/3029823180.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, kg_embedding, kg_embedding_mask, kg_embedding_mask_qid, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m         return self.transformer(\n\u001b[1;32m    568\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mposition_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mword_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, max_seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m         return F.embedding(\n\u001b[1;32m    161\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/kgml/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper__index_select)"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize MLM pipeline\n",
    "mlm = pipeline('fill-mask', model=BERTModified_model, tokenizer=my_tokenizer)\n",
    "\n",
    "# Get mask token\n",
    "mask = mlm.tokenizer.mask_token\n",
    "\n",
    "# Get result for particular masked phrase\n",
    "phrase = f'Wikipedia is a free online {mask}, created and edited by volunteers around the world'\n",
    "\n",
    "result = mlm(phrase)\n",
    "\n",
    "# Print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1df14eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in result:\n",
    "    print(f\">>> {x['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c32330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf6883f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0d424",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Add a layer for KG embedding, add the losses\n",
    "- new tokenizer for test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
